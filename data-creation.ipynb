{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12942475,"sourceType":"datasetVersion","datasetId":8190330},{"sourceId":13019707,"sourceType":"datasetVersion","datasetId":8243275},{"sourceId":13522139,"sourceType":"datasetVersion","datasetId":8586003},{"sourceId":13897154,"sourceType":"datasetVersion","datasetId":8853981},{"sourceId":13908968,"sourceType":"datasetVersion","datasetId":8862189}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data creation","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ---------- CONFIG ----------\nFPS = 25\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n# file_path = '/content/drive/My Drive/datasets/my_data.csv'\n# df = pd.read_csv(file_path)\ntracks_dir = Path(\"/content/drive/My Drive/tno/data/\")   # folder with 01_tracks.xlsx ... 60_tracks.xlsx\noutput_dir = Path(\"/content/drive/My Drive/tno/processed/merged_tracks_multi_hot\")\njson_path = Path(\"/content/drive/My Drive/tno/data/all_scenarios.json\")\n\noutput_dir.mkdir(exist_ok=True)\n# ----------------------------","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Labelling highD CSV using StreetWise created frame-level labels (json file) ","metadata":{}},{"cell_type":"code","source":"# Load scenarios JSON\nwith open(json_path, \"r\") as f:\n    scenarios = json.load(f)\n\n# Collect all unique scenario categories across datasets\nall_categories = set()\nfor _, categories in scenarios.items():\n    all_categories.update(categories.keys())\nall_categories = sorted(all_categories)  # fixed column order\n\ndef expand_scenarios_for_dataset(categories, fps=FPS):\n    \"\"\"Expand one dataset's scenarios to frame-level DataFrame (multi-hot).\"\"\"\n    records = []\n    for category, items in categories.items():\n        for item in items:\n            frame_start = int(item[\"tstart\"] * fps)\n            frame_end = int(item[\"tend\"] * fps)\n            ego_id = item.get(\"ego_id\")\n            for frame in range(frame_start, frame_end + 1):\n                records.append({\n                    \"frame\": frame,\n                    \"id\": ego_id,\n                    \"scenario\": category\n                })\n    df = pd.DataFrame(records)\n    if df.empty:\n        return df\n\n    # Pivot to multi-hot format\n    df[\"value\"] = 1\n    df = df.pivot_table(index=[\"frame\", \"id\"],\n                        columns=\"scenario\",\n                        values=\"value\",\n                        fill_value=0)\n    df = df.reset_index()\n    return df\n\n# Store for global concatenation\nall_datasets = []\n\n# Loop through all datasets in JSON\nfor dataset_id_str, categories in scenarios.items():\n  if dataset_id_str in [\"56\"]:\n    dataset_id = int(dataset_id_str)\n    track_file = tracks_dir / f\"{dataset_id:02d}_tracks.csv\"\n    if not track_file.exists():\n        print(f\"⚠️ Missing {track_file}, skipping\")\n        continue\n\n    # Load trajectory\n    tracks_df = pd.read_csv(track_file)\n\n    # Expand JSON scenarios into multi-hot labels\n    scenario_df = expand_scenarios_for_dataset(categories, fps=FPS)\n\n    # Ensure all scenario columns exist\n    for cat in all_categories:\n        if cat not in scenario_df.columns:\n            scenario_df[cat] = 0\n\n    # Merge on frame + id\n    merged = tracks_df.merge(scenario_df, on=[\"frame\", \"id\"], how=\"left\")\n\n    # Fill NaN for frames with no scenario\n    merged[all_categories] = merged[all_categories].fillna(0).astype(int)\n\n    # Save per dataset\n    out_path = output_dir / f\"{dataset_id:02d}_tracks_with_multi_hot.csv\"\n    merged.to_csv(out_path, index=False)\n    print(f\"✅ Saved per-dataset merged file: {out_path}\")\n\n    # Add dataset_id for concatenated version\n    merged[\"dataset_id\"] = dataset_id\n    all_datasets.append(merged)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Concatenating CSV files","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Pick the first 5 files\nfiles_to_concat = [\n    (i, output_dir / f\"{i:02d}_tracks_with_multi_hot.csv\")\n    for i in range(53, 57)  # 01–05\n]\n\ndfs = []\nfor doc_id, f in files_to_concat:\n    if f.exists():\n        df = pd.read_csv(f)\n        # Add document_id column first\n        cols = df.columns.tolist()\n        if \"id\" in cols:\n            idx = cols.index(\"id\")\n            cols.insert(idx, \"document_id\")\n            df[\"document_id\"] = doc_id\n            df = df[cols]  # reorder columns\n        else:\n            # If no 'id' column, just add at the start\n            df.insert(0, \"document_id\", doc_id)\n        dfs.append(df)\n    else:\n        print(f\"⚠️ Missing {f}, skipping\")\n\n# Concatenate\nif dfs:\n    combined_5 = pd.concat(dfs, ignore_index=True)\n    out_path_5 = output_dir / \"53-57_tracks_with_multi_hot.csv\"\n    combined_5.to_csv(out_path_5, index=False)\n    print(f\"✅ Saved concatenated file for first 5 datasets with document_id before id: {out_path_5}\")\nelse:\n    print(\"⚠️ No files found to concatenate.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pre-processing/cleaning CSV and converting to Parquet","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport joblib\nimport glob # Added glob for file listing in the next script logic\n\nfine_tuning=True\ntesting_f=True\n\ntracks_dir = Path(\"/kaggle/input/data-tno/data\")\nfine_file = Path(\"/kaggle/input/01-03-tracks-with-multi-hot/01-03_tracks_with_multi_hot.csv\")\ntesting_file = Path(\"/kaggle/input/57-61-tracks-with-multi-hot/57-61_tracks_with_multi_hot.csv\")\n\n\noutput_dir = Path(\"/kaggle/working/processed_parquet\")\noutput_dir.mkdir(exist_ok=True)\n\nSCALER_MAIN_PATH = \"/kaggle/working/scaler_main.joblib\"\nSCALER_PREC_PATH = \"/kaggle/working/scaler_prec.joblib\"\n\nnumerical_features = [\n    'x','y','xVelocity','yVelocity','xAcceleration','yAcceleration',\n    'frontSightDistance','backSightDistance','dhw','thw','ttc'\n]\n\nvehicle_cols = [\n    'precedingId','followingId','leftPrecedingId','rightPrecedingId',\n    'leftFollowingId','rightFollowingId','leftAlongsideId','rightAlongsideId'\n]\n\nscaler_main = StandardScaler()\nscaler_prec = StandardScaler()\n\n# ---------------------------\n# PASS 1 — fit scalers across all 52 raw CSV files\n# ---------------------------\nfor dataset_id in range(3, 55):\n    fp = tracks_dir / f\"{dataset_id:02d}_tracks.csv\"\n    if not fp.exists(): continue\n    \n    df = pd.read_csv(fp, usecols=[\n        *numerical_features, 'precedingId','precedingXVelocity'\n    ])\n\n    # fix negative velocities\n    mask = df[\"xVelocity\"] < 0\n    if mask.any():\n        df.loc[mask, [\"xVelocity\",\"yVelocity\",\"xAcceleration\",\"yAcceleration\",\"precedingXVelocity\"]] *= -1\n\n    # update main numerical scaler\n    scaler_main.partial_fit(df[numerical_features])\n\n    # preceding velocity partial fit (convert NaN to 0)\n    prec = df['precedingXVelocity'].where(df['precedingId']>0, np.nan)\n    scaler_prec.partial_fit(np.nan_to_num(prec.values.reshape(-1,1), nan=0))\n\nprint(\"✔ Scalors fitted over all 52 train files\")\n\n# ---------------------------\n# NEW: Save the fitted scalers for export\n# ---------------------------\ntry:\n    joblib.dump(scaler_main, SCALER_MAIN_PATH)\n    joblib.dump(scaler_prec, SCALER_PREC_PATH)\n    print(f\"✔ Scaler_main exported to: {SCALER_MAIN_PATH}\")\n    print(f\"✔ Scaler_prec exported to: {SCALER_PREC_PATH}\")\nexcept Exception as e:\n    print(f\"❌ Error exporting scalers: {e}\")\n\n\n# ---------------------------\n# PASS 2 — PROCESS AND SAVE TRAIN FILES (Features only, with 0, 1, ... 19 columns)\n# ---------------------------\nfor dataset_id in range(3, 55):\n# for dataset_id in range(3, 15):\n    fp = tracks_dir / f\"{dataset_id:02d}_tracks.csv\"\n    if not fp.exists(): continue\n    \n    df = pd.read_csv(fp)\n\n    # negative velocity fix\n    mask = df[\"xVelocity\"] < 0\n    if mask.any():\n        df.loc[mask, [\"xVelocity\",\"yVelocity\",\"xAcceleration\",\"yAcceleration\",\"precedingXVelocity\"]] *= -1\n\n    # lane remapping\n    unique_lanes = set(df[\"laneId\"].unique())\n    if unique_lanes == {2,3,5,6}:\n        df['laneId'] = df['laneId'].replace({2:6, 3:5})\n    elif unique_lanes == {2,3,4,6,7,8}:\n        df['laneId'] = df['laneId'].replace({2:8,3:7,4:6})\n\n    # Build features\n    X = df[numerical_features].copy()\n\n    # add binary indicators\n    for col in vehicle_cols:\n        X['has_'+col.replace(\"Id\",\"\")] = (df[col] > 0).astype(int)\n\n    # scale numerical\n    X_scaled_num = scaler_main.transform(X[numerical_features])\n\n    # scale preceding velocity\n    prec = df['precedingXVelocity'].where(df['precedingId']>0, np.nan)\n    prec_scaled = scaler_prec.transform(np.nan_to_num(prec.values.reshape(-1,1), nan=0))\n\n    # binary cols\n    binary_cols = [c for c in X.columns if c.startswith(\"has_\")]\n\n    # final stacked feature matrix (20 features)\n    final = np.hstack([X_scaled_num, prec_scaled, X[binary_cols].values])\n    \n    # --- START OF CHANGE FOR TRAIN FILES ---\n    # Create DataFrame WITHOUT providing column names. Pandas uses 0, 1, 2, ...\n    pd.DataFrame(final).to_parquet( # Removed columns=out_cols\n        output_dir / f\"{dataset_id:02d}_tracks_preprocessed.parquet\",\n        index=False\n    )\n    # --- END OF CHANGE FOR TRAIN FILES ---\n\n    print(\"✔ Saved train parquet:\", dataset_id)\n\n\n# ---------------------------\n# PROCESS AND SAVE FINE-TUNE FILE (Features + Labels)\n# ---------------------------\n\nif fine_tuning:\n    df_main = pd.read_csv(fine_file)\n    # target_cols = df_main.columns[26:38]\n    target_cols=df_main.columns[26:38].delete([28-26, 33-26])\n    \n    df = df_main[~df_main[target_cols].eq(0).all(axis=1)].copy()\n    \n    # negative velocities\n    mask = df[\"xVelocity\"] < 0\n    if mask.any():\n        df.loc[mask, [\"xVelocity\",\"yVelocity\",\"xAcceleration\",\"yAcceleration\",\"precedingXVelocity\"]] *= -1\n    \n    # lane remapping\n    unique_lanes = set(df[\"laneId\"].unique())\n    if unique_lanes == {2,3,5,6}:\n        df['laneId'] = df['laneId'].replace({2:6,3:5})\n    elif unique_lanes == {2,3,4,6,7,8}:\n        df['laneId'] = df['laneId'].replace({2:8,3:7,4:6})\n    \n    # Features\n    X = df[numerical_features].copy()\n    for col in vehicle_cols:\n        X['has_'+col.replace(\"Id\",\"\")] = (df[col] > 0).astype(int)\n    \n    # scale numerical\n    X_scaled_num = scaler_main.transform(X[numerical_features])\n    \n    # scale preceding velocity\n    prec = df['precedingXVelocity'].where(df['precedingId']>0, np.nan)\n    prec_scaled = scaler_prec.transform(np.nan_to_num(prec.values.reshape(-1,1), nan=0))\n    \n    binary_cols = [c for c in X.columns if c.startswith(\"has_\")]\n    \n    # labels\n    # scenario_cols = df.columns[26:38]\n    # Y = df[scenario_cols].values\n    scenario_cols = target_cols\n    Y = df[scenario_cols].values\n    \n    # final matrix features are stacked first\n    final_matrix = np.hstack([\n        df[['document_id','id','frame']].values, # 3 columns\n        X_scaled_num,                            # 11 numerical + 1 prec_scaled = 12 columns\n        prec_scaled,                             # Combined into X_scaled_num above (1 column)\n        X[binary_cols].values,                   # 8 binary columns\n        Y                                        # 12 label columns\n    ])\n    \n    # The feature columns (0 to 19) are X_scaled_num (11) + prec_scaled (1) + X[binary_cols].values (8)\n    \n    # The total number of columns in final_matrix is 3 (IDs) + 20 (Features) + 12 (Labels) = 35\n    \n    # --- START OF CHANGE FOR FINE-TUNE FILE ---\n    # 1. Define column names ONLY for the non-feature data (IDs and Labels).\n    id_cols = ['document_id','id','frame']\n    label_cols = list(scenario_cols)\n    \n    # 2. Generate the integer column names for the 20 features (0 to 19)\n    feature_cols = [i for i in range(20)]\n    \n    # 3. Create the final column list\n    final_cols = id_cols + feature_cols + label_cols\n    \n    # 4. Use the combined list to create the DataFrame\n    df_out = pd.DataFrame(final_matrix, columns=final_cols)\n    df_out.to_parquet(output_dir / \"fine_tuning_preprocessed.parquet\", index=False)\n    # --- END OF CHANGE FOR FINE-TUNE FILE ---\n    \n    print(\"✔ Saved FINE-TUNE parquet\")\n\nif testing_f:\n    df_main = pd.read_csv(testing_file)\n    # target_cols = df_main.columns[26:38]\n    target_cols=df_main.columns[26:38].delete([28-26, 33-26])\n    \n    df = df_main[~df_main[target_cols].eq(0).all(axis=1)].copy()\n    \n    # negative velocities\n    mask = df[\"xVelocity\"] < 0\n    if mask.any():\n        df.loc[mask, [\"xVelocity\",\"yVelocity\",\"xAcceleration\",\"yAcceleration\",\"precedingXVelocity\"]] *= -1\n    \n    # lane remapping\n    unique_lanes = set(df[\"laneId\"].unique())\n    if unique_lanes == {2,3,5,6}:\n        df['laneId'] = df['laneId'].replace({2:6,3:5})\n    elif unique_lanes == {2,3,4,6,7,8}:\n        df['laneId'] = df['laneId'].replace({2:8,3:7,4:6})\n    \n    # Features\n    X = df[numerical_features].copy()\n    for col in vehicle_cols:\n        X['has_'+col.replace(\"Id\",\"\")] = (df[col] > 0).astype(int)\n    \n    # scale numerical\n    X_scaled_num = scaler_main.transform(X[numerical_features])\n    \n    # scale preceding velocity\n    prec = df['precedingXVelocity'].where(df['precedingId']>0, np.nan)\n    prec_scaled = scaler_prec.transform(np.nan_to_num(prec.values.reshape(-1,1), nan=0))\n    \n    binary_cols = [c for c in X.columns if c.startswith(\"has_\")]\n    \n    # labels\n    # scenario_cols = df.columns[26:38]\n    # Y = df[scenario_cols].values\n    scenario_cols = target_cols\n    Y = df[scenario_cols].values\n    \n    # final matrix features are stacked first\n    final_matrix = np.hstack([\n        df[['document_id','id','frame']].values, # 3 columns\n        X_scaled_num,                            # 11 numerical + 1 prec_scaled = 12 columns\n        prec_scaled,                             # Combined into X_scaled_num above (1 column)\n        X[binary_cols].values,                   # 8 binary columns\n        Y                                        # 12 label columns\n    ])\n    \n    # The feature columns (0 to 19) are X_scaled_num (11) + prec_scaled (1) + X[binary_cols].values (8)\n    \n    # The total number of columns in final_matrix is 3 (IDs) + 20 (Features) + 12 (Labels) = 35\n    \n    # --- START OF CHANGE FOR FINE-TUNE FILE ---\n    # 1. Define column names ONLY for the non-feature data (IDs and Labels).\n    id_cols = ['document_id','id','frame']\n    label_cols = list(scenario_cols)\n    \n    # 2. Generate the integer column names for the 20 features (0 to 19)\n    feature_cols = [i for i in range(20)]\n    \n    # 3. Create the final column list\n    final_cols = id_cols + feature_cols + label_cols\n    \n    # 4. Use the combined list to create the DataFrame\n    df_out = pd.DataFrame(final_matrix, columns=final_cols)\n    df_out.to_parquet(output_dir / \"testing_preprocessed.parquet\", index=False)\n    # --- END OF CHANGE FOR FINE-TUNE FILE ---\n    \n    print(\"✔ Saved testing parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:26:04.846821Z","iopub.execute_input":"2026-01-10T15:26:04.847135Z","iopub.status.idle":"2026-01-10T15:29:18.849910Z","shell.execute_reply.started":"2026-01-10T15:26:04.847112Z","shell.execute_reply":"2026-01-10T15:29:18.849001Z"}},"outputs":[{"name":"stdout","text":"✔ Scalors fitted over all 52 train files\n✔ Scaler_main exported to: /kaggle/working/scaler_main.joblib\n✔ Scaler_prec exported to: /kaggle/working/scaler_prec.joblib\n✔ Saved train parquet: 3\n✔ Saved train parquet: 4\n✔ Saved train parquet: 5\n✔ Saved train parquet: 6\n✔ Saved train parquet: 7\n✔ Saved train parquet: 8\n✔ Saved train parquet: 9\n✔ Saved train parquet: 10\n✔ Saved train parquet: 11\n✔ Saved train parquet: 12\n✔ Saved train parquet: 13\n✔ Saved train parquet: 14\n✔ Saved train parquet: 15\n✔ Saved train parquet: 16\n✔ Saved train parquet: 17\n✔ Saved train parquet: 18\n✔ Saved train parquet: 19\n✔ Saved train parquet: 20\n✔ Saved train parquet: 21\n✔ Saved train parquet: 22\n✔ Saved train parquet: 23\n✔ Saved train parquet: 24\n✔ Saved train parquet: 25\n✔ Saved train parquet: 26\n✔ Saved train parquet: 27\n✔ Saved train parquet: 28\n✔ Saved train parquet: 29\n✔ Saved train parquet: 30\n✔ Saved train parquet: 31\n✔ Saved train parquet: 32\n✔ Saved train parquet: 33\n✔ Saved train parquet: 34\n✔ Saved train parquet: 35\n✔ Saved train parquet: 36\n✔ Saved train parquet: 37\n✔ Saved train parquet: 38\n✔ Saved train parquet: 39\n✔ Saved train parquet: 40\n✔ Saved train parquet: 41\n✔ Saved train parquet: 42\n✔ Saved train parquet: 43\n✔ Saved train parquet: 44\n✔ Saved train parquet: 45\n✔ Saved train parquet: 46\n✔ Saved train parquet: 47\n✔ Saved train parquet: 48\n✔ Saved train parquet: 49\n✔ Saved train parquet: 50\n✔ Saved train parquet: 51\n✔ Saved train parquet: 52\n✔ Saved train parquet: 53\n✔ Saved train parquet: 54\n✔ Saved FINE-TUNE parquet\n✔ Saved testing parquet\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Zip the parquet files for downloading","metadata":{}},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/processed_parquet_zip\", 'zip', \"/kaggle/working/processed_parquet\")\nprint(\"✅ Zipped processed_parquet folder\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:29:18.851187Z","iopub.execute_input":"2026-01-10T15:29:18.851422Z","iopub.status.idle":"2026-01-10T15:29:50.677859Z","shell.execute_reply.started":"2026-01-10T15:29:18.851403Z","shell.execute_reply":"2026-01-10T15:29:50.677091Z"}},"outputs":[{"name":"stdout","text":"✅ Zipped processed_parquet folder\n","output_type":"stream"}],"execution_count":2}]}