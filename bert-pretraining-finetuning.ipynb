{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data loading and tfrecords creation\nMake sure to connect to GPU P100 if on kaggle environment","metadata":{}},{"cell_type":"code","source":"\"\"\"\nOnly relevant if performing ablation for BERT pre-training, \nmake sure to run it at the begining and the change hyperparamter configuration in 'main pretrained bock' and run 'plotting ablation result block'\n\"\"\"\nall_histories = {} ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf,keras\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nimport math,random,time,shutil,tempfile,glob,os,json,itertools\nfrom pathlib import Path\nfrom tensorflow.keras import layers, models, callbacks, mixed_precision,Model\nfrom tensorflow.keras.initializers import HeNormal, GlorotUniform\nfrom typing import Optional\nimport tensorflow.keras.metrics as metrics\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nfrom typing import List,Dict,Any\nfrom sklearn.metrics import f1_score, classification_report, hamming_loss\n\n\nPARQUET_DIR = \"/kaggle/input/tno-parquet-latest2\"\nTFRECORD_DIR = \"tfrecords_highd\"\nprocessed_dir = \"/kaggle/working/\"\nSEQ_SIZES = [64, 128, 192]  # ~2s, ~5s, ~7s\nBATCH_SIZES = [256, 128, 64]\nFEATURES = [str(i) for i in range(0,20)]\nNUM_FEATURES = len(FEATURES)\nTARGET_SHARD_MEMORY_MB = 20\nBYTES_PER_FLOAT = 4\nMASK_PROB = 0.9\nAUTOTUNE = tf.data.AUTOTUNE\nos.environ['PYTHONHASHSEED'] = '42'\nrandom.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)\nos.makedirs(TFRECORD_DIR, exist_ok=True) \nmixed_precision.set_global_policy(\"mixed_float16\")\n# mixed_precision.set_global_policy(\"float32\")\n\nCONTAINER_LEN = None  # Must match pre-trained BERT input size\nACTIVE_LEN = None  \nLABEL_COLS=None\nNUM_LABELS = None\nSTRIDE = None\n\n\"\"\"\nVery important!!\nset the below parameter as per your expectations\n\"\"\"\nUSE_ABLATION, USE_FINE_TUNE, testing, generate_file = False, False, False, False\n\n\n\ndef TFRecords_Creation(PARQUET_DIR,stride_ratio):\n    global CONTAINER_LEN,ACTIVE_LEN, LABEL_COLS, NUM_LABELS,STRIDE\n    if testing==False and USE_FINE_TUNE == False:\n        parquet_files = sorted(glob.glob(os.path.join(PARQUET_DIR, \"*.parquet\")))\n        train_files = parquet_files\n        if USE_ABLATION==False:\n            split_to_files = {\n                \"train\": train_files\n            }\n            all_sequences_by_split = {size: {\"train\": []} for size in SEQ_SIZES}\n        else:\n            split_to_files = {\n                \"train\": train_files[:43],\n                \"val\": train_files[43:48],\n                \"test\": train_files[48:]\n            }\n            all_sequences_by_split = {size: {\"train\": [],\"val\": [],\"test\": []} for size in SEQ_SIZES}\n        \n        for split, files in split_to_files.items():\n            for parquet_file in files:\n                df = pd.read_parquet(parquet_file)\n                arrays = {f: df[f].to_numpy() for f in FEATURES}\n                for size in SEQ_SIZES:\n                    stride = int(size*stride_ratio)\n                    for start_idx in range(0, len(df) - size + 1, stride):\n                        seq = np.stack([arrays[f][start_idx:start_idx+size] for f in FEATURES], axis=-1)\n                        all_sequences_by_split[size][split].append(seq)\n        \n        print(\"Collected sequences per split.\")\n        \n        # compute shard_info per split and size, then write into subfolders\n        def serialize_example(sequence):\n            feature = {\n                \"features\": tf.train.Feature(float_list=tf.train.FloatList(value=sequence.flatten()))\n            }\n            example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n            return example_proto.SerializeToString()\n        \n        for size in SEQ_SIZES:\n            for split in split_to_files.keys():\n                sequences = all_sequences_by_split[size][split]\n                total_sequences = len(sequences)\n                seq_memory_bytes = size * NUM_FEATURES * BYTES_PER_FLOAT\n                target_shard_bytes = TARGET_SHARD_MEMORY_MB * 1024 * 1024\n                seq_per_shard = max(1, int(target_shard_bytes / seq_memory_bytes))\n                num_shards = math.ceil(total_sequences / seq_per_shard)\n        \n                out_dir = os.path.join(TFRECORD_DIR, split)\n                os.makedirs(out_dir, exist_ok=True)\n        \n                for shard_id in range(num_shards):\n                    shard_filename = os.path.join(out_dir, f\"{split}_seq{size}_{shard_id:03d}.tfrecord\")\n                    start = shard_id * seq_per_shard\n                    end = min((shard_id + 1) * seq_per_shard, total_sequences)\n                    with tf.io.TFRecordWriter(shard_filename) as writer:\n                        for seq in sequences[start:end]:\n                            writer.write(serialize_example(seq))\n                print(f\"Wrote {num_shards} shards for split={split}, seq_len={size}, total_seqs={total_sequences}\")\n    \n    else:\n        def _bytes_feature(value):\n            \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n            # Ensure value is encoded as bytes if it's a string\n            if isinstance(value, str):\n                value = value.encode('utf-8')\n            return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n        \n        def _float_feature(value):\n            \"\"\"Returns a float_list from a float or list of floats.\"\"\"\n            return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n        \n        def serialize(x, y, mask, doc_id, veh_id):\n            \"\"\"\n            Serializes sequence data (x, y, mask) and string IDs (doc_id, veh_id)\n            into a tf.train.Example.\n            \"\"\"\n            return tf.train.Example(features=tf.train.Features(feature={\n                # Sequence data (float_list)\n                \"x\": _float_feature(x.flatten()),\n                \"y\": _float_feature(y.flatten()),\n                \"mask\": _float_feature(mask.flatten()),\n                # ID data (bytes_list)\n                \"doc_id\": _bytes_feature(doc_id),\n                \"veh_id\": _bytes_feature(veh_id),\n            })).SerializeToString()\n        \n        def write_split(groups, split_name):\n            \"\"\"\n            Processes groups of DataFrame rows, segments them into sequences,\n            and writes them to a TFRecord file, including document and vehicle IDs.\n            \"\"\"\n            # Lists to hold sequence data\n            sequences, labels, masks = [], [], []\n            # Lists to hold ID data (used for verification/logging if needed, but primarily\n            # passed to serialize() now)\n            doc_ids_new, veh_ids_new = [], []\n        \n            container_len, active_len, stride = CONTAINER_LEN, ACTIVE_LEN, STRIDE\n        \n            for (d, v), group_df in groups:\n                # 'd' is document_id, 'v' is vehicle_id\n                # group_df is the DataFrame slice for that specific (doc, veh) pair\n        \n                # Extract features and labels\n                X = group_df[FEATURES].to_numpy().astype(\"float32\")\n                Y = group_df[LABEL_COLS].to_numpy().astype(\"float32\")\n        \n                T = len(group_df)\n                static_pad = container_len - active_len\n        \n                # --- Logic for testing==True (25-frame chunks) ---\n                if testing == True:\n                    X1, Y1 = [], []\n                    for start in range(0, T - 25 + 1, 25):\n                        X1.extend(X[start : start + 25])\n                        Y1.extend(Y[start : start + 25])\n        \n                    len_X1 = len(X1)\n        \n                    # Handle sequences shorter than active_len\n                    if len_X1 < active_len:\n                        active_pad = active_len - len_X1\n                        total_pad = active_pad + static_pad\n        \n                        x = np.pad(X1, ((0, total_pad), (0, 0)), mode='constant', constant_values=0.0)\n                        y = np.pad(Y1, ((0, total_pad), (0, 0)), mode='constant', constant_values=0.0)\n                        mask = np.concatenate([np.ones(len_X1, dtype=np.float32), np.zeros(total_pad, dtype=np.float32)])\n        \n                        sequences.append(x)\n                        labels.append(y)\n                        masks.append(mask)\n                        doc_ids_new.append(d)\n                        veh_ids_new.append(v)\n        \n                    # Handle sequences longer than active_len (sliding window)\n                    else:\n                        X1 = np.array(X1)\n                        Y1 = np.array(Y1)\n                        \n                        # Check for padding needed to align with stride for the last window\n                        remaining = (len_X1 - active_len) % stride\n                        if remaining != 0:\n                            s = stride - remaining\n                            X1 = np.pad(X1, ((0, s), (0, 0)), mode='constant', constant_values=0.0)\n                            Y1 = np.pad(Y1, ((0, s), (0, 0)), mode='constant', constant_values=0.0)\n                        else:\n                            s = 0\n        \n                        num_windows = (len(X1) - active_len) // stride + 1\n        \n                        for count, start in enumerate(range(0, len(X1) - active_len + 1, stride)):\n                            x_slice = X1[start : start + active_len]\n                            y_slice = Y1[start : start + active_len]\n        \n                            x_final = np.pad(x_slice, ((0, static_pad), (0, 0)), mode='constant', constant_values=0.0)\n                            y_final = np.pad(y_slice, ((0, static_pad), (0, 0)), mode='constant', constant_values=0.0)\n        \n                            # Determine mask based on padding for the last window\n                            if count == num_windows - 1 and remaining != 0:\n                                mask_final = np.concatenate([np.ones(active_len - s, dtype=np.float32), np.zeros(static_pad + s, dtype=np.float32)])\n                            else:\n                                mask_final = np.concatenate([np.ones(active_len, dtype=np.float32), np.zeros(static_pad, dtype=np.float32)])\n        \n                            sequences.append(x_final)\n                            labels.append(y_final)\n                            masks.append(mask_final)\n                            doc_ids_new.append(d)\n                            veh_ids_new.append(v)\n        \n                # --- Logic for testing==False (full group processing) ---\n                else: # training/validation logic\n                    len_X = len(X)\n                    # Handle groups shorter than active_len\n                    if len_X < active_len:\n                        active_pad = active_len - len_X\n                        total_pad = active_pad + static_pad\n        \n                        x = np.pad(X, ((0, total_pad), (0, 0)), mode='constant', constant_values=0.0)\n                        y = np.pad(Y, ((0, total_pad), (0, 0)), mode='constant', constant_values=0.0)\n                        mask = np.concatenate([np.ones(len_X, dtype=np.float32), np.zeros(total_pad, dtype=np.float32)])\n        \n                        sequences.append(x)\n                        labels.append(y)\n                        masks.append(mask)\n                        doc_ids_new.append(d)\n                        veh_ids_new.append(v)\n                    \n                    # Handle groups longer than active_len (sliding window)\n                    else:\n                        # Check for padding needed to align with stride for the last window\n                        remaining = (len_X - active_len) % stride\n                        if remaining != 0:\n                            s = stride - remaining\n                            X = np.pad(X, ((0, s), (0, 0)), mode='constant', constant_values=0.0)\n                            Y = np.pad(Y, ((0, s), (0, 0)), mode='constant', constant_values=0.0)\n                        else:\n                            s = 0\n        \n                        num_windows = (len(X) - active_len) // stride + 1\n        \n                        for count, start in enumerate(range(0, len(X) - active_len + 1, stride)):\n                            x_slice = X[start : start + active_len]\n                            y_slice = Y[start : start + active_len]\n        \n                            x_final = np.pad(x_slice, ((0, static_pad), (0, 0)), mode='constant', constant_values=0.0)\n                            y_final = np.pad(y_slice, ((0, static_pad), (0, 0)), mode='constant', constant_values=0.0)\n                            \n                            # Determine mask based on padding for the last window\n                            if count == num_windows - 1 and remaining != 0:\n                                mask_final = np.concatenate([np.ones(active_len - s, dtype=np.float32), np.zeros(static_pad + s, dtype=np.float32)])\n                            else:\n                                mask_final = np.concatenate([np.ones(active_len, dtype=np.float32), np.zeros(static_pad, dtype=np.float32)])\n        \n                            sequences.append(x_final)\n                            labels.append(y_final)\n                            masks.append(mask_final)\n                            doc_ids_new.append(d)\n                            veh_ids_new.append(v)\n        \n        \n            out_file = f\"{TFRECORD_DIR}/{split_name}.tfrecord\"\n            tf.io.gfile.makedirs(TFRECORD_DIR) # Ensure directory exists\n            \n            with tf.io.TFRecordWriter(out_file) as w:\n                # Pass the IDs to the serialize function along with the sequence data\n                for x, y, m, d, v in zip(sequences, labels, masks, doc_ids_new, veh_ids_new):\n                    doc_id_safe = str(d)\n                    veh_id_safe = str(v)\n                    \n                    # Pass the corrected string IDs to the serialize function\n                    w.write(serialize(x, y, m, doc_id_safe, veh_id_safe))\n                    \n            # The return signature now depends on the split_name (or testing mode)\n            if split_name == \"test\" or testing == True:\n                return out_file, doc_ids_new, veh_ids_new\n            else:\n                return out_file\n        \n        # --- Data Grouping and Splitting Logic ---\n        # Note: The groups list must now contain (key, DataFrame) tuples\n        groups = []\n        \n        if testing==True:\n            df = pd.read_parquet(PARQUET_DIR[1])\n            CONTAINER_LEN = 192  # Must match pre-trained BERT input size\n            ACTIVE_LEN = 192  \n            LABEL_COLS=df.columns[23:].tolist()\n            NUM_LABELS = len(LABEL_COLS)\n            STRIDE = int(ACTIVE_LEN * 1.0)\n            # IMPORTANT CORRECTION: Append the key along with the DataFrame\n            for key, g in df.groupby([\"document_id\", \"id\"], sort=False):\n                if len(g)<25:\n                    continue\n                groups.append((key, g.sort_values(\"frame\")))\n                \n            # Correctly unpack the multiple return values\n            test_tfr, doc_ids_new, veh_ids_new = write_split(groups, \"test\")\n            print(f\"Test TFRecord written to: {test_tfr}\")\n            print(f\"Total sequences written: {len(doc_ids_new)}\")\n            # print(f\"Example Doc IDs: {doc_ids_new[:5]}\") # Print for verification\n            # print(f\"Example Veh IDs: {veh_ids_new[:5]}\") # Print for verification\n        \n        \n        if testing==False:\n            df = pd.read_parquet(PARQUET_DIR[0])\n            \n            \n            CONTAINER_LEN = 192  # Must match pre-trained BERT input size\n            ACTIVE_LEN = 192  \n            LABEL_COLS=df.columns[23:].tolist()\n            NUM_LABELS = len(LABEL_COLS)\n            STRIDE = int(ACTIVE_LEN * stride_ratio)\n            for key, g in df.groupby([\"document_id\", \"id\"], sort=False):\n                groups.append((key, g.sort_values(\"frame\")))\n        \n            # 2. Perform the Split (Exact match to BiLSTM logic)\n            # Step A: Split off 15% for TEST\n            train_val_groups, test_groups = train_test_split(\n                groups, test_size=0.15, random_state=42\n            )\n            \n            # Step B: Split the remaining 85% into TRAIN (70% total) and VAL (15% total)\n            val_relative_size = 0.15 / 0.85\n            \n            train_groups, val_groups = train_test_split(\n                train_val_groups, test_size=val_relative_size, random_state=42\n            )\n            \n            print(f\"Train groups: {len(train_groups)} (~{len(train_groups)/len(groups):.1%})\")\n            print(f\"Val groups:    {len(val_groups)} (~{len(val_groups)/len(groups):.1%})\")\n            print(f\"Test groups:   {len(test_groups)} (~{len(test_groups)/len(groups):.1%})\")\n        \n            # The write_split for train/val returns only the file path\n            train_tfr = write_split(train_groups, \"train\")\n            val_tfr   = write_split(val_groups, \"val\")\n            test_tfr, _, _ = write_split(test_groups, \"test\") # Ignore IDs for train/val/test split mode\n            \n            print(f\"TFRecord files written: Train: {train_tfr}, Val: {val_tfr}, Test: {test_tfr}\")\n        \n        print(\"TFRecord generation complete.\")\n\nif generate_file==True:\n    if testing ==False and USE_FINE_TUNE == False:\n        TFRecords_Creation(\"/kaggle/input/tno-parquet-latest2\", 0.8)\n        shutil.make_archive(\"/kaggle/working/tfrecords_highd\", \"zip\", processed_dir)\n        \n    elif USE_FINE_TUNE == True:\n        TFRecords_Creation([\"/kaggle/input/fine-tuning-preprocessed/fine_tuning_preprocessed.parquet\",\"/kaggle/input/testing-preprocessed/testing_preprocessed.parquet\"], 0.8)\n        shutil.make_archive(\"/kaggle/working/tfrecords_highd\", \"zip\", processed_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T11:27:04.891689Z","iopub.execute_input":"2026-01-15T11:27:04.892511Z","iopub.status.idle":"2026-01-15T11:27:12.766858Z","shell.execute_reply.started":"2026-01-15T11:27:04.892470Z","shell.execute_reply":"2026-01-15T11:27:12.766259Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# BERT pre-training ","metadata":{}},{"cell_type":"markdown","source":"## Main pre-training block: tfrecords pre-processing and BERT defination","metadata":{}},{"cell_type":"code","source":"if USE_FINE_TUNE==False:\n    if USE_ABLATION:\n        MAX_SEQ_LEN = 64\n    else:\n        MAX_SEQ_LEN = max(SEQ_SIZES)\n    \n    def parse_and_pad_example(example_proto, seq_len):\n        feature_description = {\"features\": tf.io.FixedLenFeature([seq_len * NUM_FEATURES], tf.float32)}\n        parsed = tf.io.parse_single_example(example_proto, feature_description)\n        seq = tf.reshape(parsed[\"features\"], (seq_len, NUM_FEATURES))\n        pad_amount = MAX_SEQ_LEN - seq_len  # Python int\n        if pad_amount < 0:\n            raise ValueError(f\"seq_len {seq_len} > MAX_SEQ_LEN {MAX_SEQ_LEN}\")\n        seq = tf.pad(seq, [[0, pad_amount], [0, 0]])\n        valid_mask = tf.concat([tf.ones(seq_len, dtype=tf.float32), tf.zeros(pad_amount, dtype=tf.float32)], axis=0)\n        return seq, valid_mask\n    \n    def make_base_dataset(seq_len, batch_size, subset):\n        \"\"\"\n        Creates a batched and prefetched tf.data.Dataset for the given subset.\n        Handles both full and ablation modes.\n        \"\"\"\n        if subset==\"train\":\n            tfrecord_files = sorted(glob.glob(os.path.join(f\"/kaggle/input/tfrecords20-{subset}2\", f\"{subset}_seq{seq_len}_*.tfrecord\")))\n        else:\n            tfrecord_files = sorted(glob.glob(os.path.join(f\"/kaggle/input/tfrecords-{subset}\", f\"{subset}_seq{seq_len}_*.tfrecord\")))\n            \n        if USE_ABLATION:\n            if subset==\"train\":\n                print(\"train\")\n                tfrecord_files = [os.path.join(f\"/kaggle/input/tfrecords-{subset}\", f\"{subset}_seq{seq_len}_{i:03d}.tfrecord\") for i in range(0, 50)]\n            elif subset==\"val\":\n                print(\"val\")\n                tfrecord_files = [os.path.join(f\"/kaggle/input/tfrecords-{subset}\", f\"{subset}_seq{seq_len}_{i:03d}.tfrecord\") for i in range(0, 5)]\n            else:\n                print(\"test\")\n                tfrecord_files = [os.path.join(f\"/kaggle/input/tfrecords-{subset}\", f\"{subset}_seq{seq_len}_{i:03d}.tfrecord\") for i in range(0, 5)]\n    \n        # Build dataset\n        ds = tf.data.TFRecordDataset(tfrecord_files, num_parallel_reads=AUTOTUNE)\n        ds = ds.map(lambda x: parse_and_pad_example(x, seq_len), num_parallel_calls=AUTOTUNE)\n        ds = ds.shuffle(2000, seed=42).batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)\n        return ds\n    \n    # Masked Frame Modeling applied on batches (vectorized)\n    def apply_mfm_on_batch(batch_seq, padding_mask):\n        \"\"\"\n        batch_seq: [B, MAX_SEQ_LEN, F]\n        padding_mask: [B, MAX_SEQ_LEN] float\n        \"\"\"\n        batch_size = tf.shape(batch_seq)[0]\n        \n        # mask_prob=MASK_PROB if subset==\"train\" else 0.3\n        mask_prob=MASK_PROB\n        # random mask where valid\n        rnd = tf.random.uniform((batch_size, MAX_SEQ_LEN))\n        mfm_mask = tf.cast(rnd < mask_prob, tf.float32) * padding_mask\n    \n        # expand to feature dimension\n        mfm_mask_feat = tf.expand_dims(mfm_mask, -1)\n    \n        # masked input & target\n        x_masked = batch_seq * (1.0 - mfm_mask_feat)\n        y_target = batch_seq * mfm_mask_feat\n    \n        # x_masked = tf.clip_by_value(x_masked, -5.0, 5.0)\n        # y_target = tf.clip_by_value(y_target, -5.0, 5.0)\n        sample_weight = mfm_mask\n        # print(mfm_mask.shape)\n        return (x_masked, padding_mask), y_target, sample_weight\n    \n    # Build per-size datasets (these produce (inputs, y, sample_weight) tuples eventually)\n    datasets_by_size = {}\n    if USE_ABLATION:\n        # ðŸ”’ Only use seq_len = 64 for ablation study\n        print(f\"ðŸ§ª Ablation mode active: using only seq_len={MAX_SEQ_LEN} for all subsets\")\n        train_ds = make_base_dataset(MAX_SEQ_LEN, BATCH_SIZES[0], \"train\").map(lambda seqs, pads: apply_mfm_on_batch(seqs, pads), num_parallel_calls=AUTOTUNE)\n        val_ds = make_base_dataset(MAX_SEQ_LEN, BATCH_SIZES[0], \"val\").map(lambda seqs, pads: apply_mfm_on_batch(seqs, pads), num_parallel_calls=AUTOTUNE)\n        datasets_by_size = {\"train\": train_ds, \"val\": val_ds}\n    else:\n        for size, batch_size in zip(SEQ_SIZES, BATCH_SIZES):\n            # yields (seq_padded, padding_mask)\n            ds_train = make_base_dataset(size, batch_size, subset=\"train\").map(lambda seqs, pads: apply_mfm_on_batch(seqs, pads), num_parallel_calls=AUTOTUNE)\n            datasets_by_size[size] = {\"train\": ds_train} \n    \n    # --------------------------\n    # Custom Positional Embedding Layer\n    # --------------------------\n    class PositionalEmbedding(layers.Layer):\n        \"\"\"\n        Adds learned positional embeddings to the input sequence.\n        \"\"\"\n        def __init__(self, max_seq_len, d_model, **kwargs):\n            super().__init__(**kwargs)\n            self.max_seq_len = max_seq_len\n            self.d_model = d_model\n            self.pos_emb = layers.Embedding(input_dim=max_seq_len, output_dim=d_model,dtype=\"float32\")\n    \n        def call(self, x):\n            # Create position indices dynamically based on sequence length\n            seq_len = tf.shape(x)[1]\n            positions = tf.range(start=0, limit=seq_len, delta=1)\n            pos_embeddings = tf.cast(self.pos_emb(positions),x.dtype)\n            return x + pos_embeddings\n    \n        def get_config(self):\n            config = super().get_config()\n            config.update({\n                \"max_seq_len\": self.max_seq_len,\n                \"d_model\": self.d_model,\n            })\n            return config\n    \n    def padfloat_to_attnbool(m):\n        return tf.expand_dims(tf.expand_dims(tf.cast(m > 0.5, tf.bool), 1), 1)\n    \n    # --------------------------\n    # Model Builder Function\n    # --------------------------\n    # def build_model(max_seq_len=MAX_SEQ_LEN, num_features=NUM_FEATURES, d_model=64, num_heads=4, num_layers=2, dropout_rate=0.0):\n    \n    def build_model(max_seq_len=MAX_SEQ_LEN, num_features=NUM_FEATURES, d_model=256, num_heads=8, num_layers=8, dropout_rate=0.1):\n        \"\"\"\n        Builds a BERT-like Transformer model for vehicle trajectory pretraining,\n        incorporating dropout for regularization.\n        \"\"\"\n        # === Inputs ===\n        traj_in = tf.keras.Input(shape=(max_seq_len, num_features),dtype=tf.float32, name=\"trajectory\")\n        pad_mask_in = tf.keras.Input(shape=(max_seq_len,), dtype=tf.float32, name=\"pad_mask\")\n    \n        # === Input projection ===\n        x = layers.LayerNormalization(epsilon=1e-6, dtype=\"float32\",name=\"input_ln\")(traj_in)\n        x = layers.Dense(d_model, kernel_initializer=GlorotUniform(seed=42), name=\"input_proj\")(x)\n        x = layers.Activation(\"gelu\")(x)\n        x = layers.Dropout(dropout_rate, name=\"input_dropout\")(x) # <--- ADDED DROPOUT\n        # === Add positional embeddings ===\n        x = PositionalEmbedding(max_seq_len, d_model, name=\"positional_embedding\")(x)\n        \n        # === Prepare attention mask (wrapped in Keras Lambda for compatibility) ===\n        attn_mask = layers.Lambda(padfloat_to_attnbool, name=\"padfloat_to_attnbool\")(pad_mask_in)\n        \n        # === Transformer encoder blocks (Pre-Normalization) ===\n        for i in range(num_layers):\n            # 1. Attention Block\n            norm1 = layers.LayerNormalization(epsilon=1e-6,dtype=\"float32\", name=f\"attn_norm_{i}\")(x) # Pre-Normalization\n            attn_out = layers.MultiHeadAttention(\n                num_heads=num_heads,\n                key_dim=d_model // num_heads,\n                name=f\"mha_{i}\",\n                use_bias=False,\n                dtype=\"float32\"\n            )(norm1, norm1, attention_mask= attn_mask)\n            # Add a projection layer if needed (as in your original, though often omitted)\n            attn_out = layers.Dense(d_model, kernel_initializer=GlorotUniform(seed=42),dtype=\"float32\")(attn_out)\n            attn_out = layers.Dropout(dropout_rate, name=f\"attn_dropout_{i}\")(attn_out) # <--- ADDED DROPOUT\n            \n            # Residual connection\n            x = layers.Add(name=f\"attn_residual_{i}\")([x, attn_out])\n            \n            # 2. Feedforward Block\n            norm2 = layers.LayerNormalization(epsilon=1e-6,dtype=\"float32\", name=f\"ffn_norm_{i}\")(x) # Pre-Normalization\n            # ffn = layers.Dense(d_model * 4, activation=\"gelu\", kernel_initializer=GlorotUniform(seed=42), name=f\"ffn_dense1_{i}\")(norm2)\n            # ffn = layers.Dense(d_model, kernel_initializer=GlorotUniform(seed=42), name=f\"ffn_dense2_{i}\")(ffn)\n            gate = layers.Dense(int(d_model * 8 / 3) , activation=\"swish\", kernel_initializer=GlorotUniform(seed=42), name=f\"ffn_gate_{i}\",dtype=\"float32\")(norm2) \n            linear = layers.Dense(int(d_model * 8 / 3) , kernel_initializer=GlorotUniform(seed=42), name=f\"ffn_linear_{i}\",dtype=\"float32\")(norm2) \n            x_gated = layers.Multiply(name=f\"ffn_multiply_{i}\")([gate, linear]) \n            ffn = layers.Dense(d_model,kernel_initializer=GlorotUniform(seed=42), name=f\"ffn_dense_out_{i}\", dtype=\"float32\")(x_gated)\n            \n            ffn = layers.Dropout(dropout_rate, name=f\"ffn_dropout_{i}\")(ffn) # <--- ADDED DROPOUTm\n    \n            # Residual\n            x = layers.Add(name=f\"ffn_residual_{i}\")([x, ffn])\n            \n        # === Final clean-up layer (optional, but standard for Pre-Norm) ===\n        # x = layers.LayerNormalization(epsilon=1e-6,dtype=\"float32\", name=\"final_norm\")(x) # <--- FINAL NORM\n    \n        # === Output projection ===\n        # Project back to original feature dimension (num_features) for the MLM task\n        outputs = layers.Dense(num_features, dtype=\"float32\", kernel_initializer=GlorotUniform(seed=42), name=\"output_projection\")(x)\n    \n        # === Build model ===\n        model = Model(inputs=[traj_in, pad_mask_in], outputs=outputs, name=\"TrajectoryBERT\")\n        print(\"âœ… Model built successfully\")\n        return model\n        \n    curriculum_stages = [\n        ([64], [1.0]),\n        ([64, 128], [0.7, 0.3]),\n        ([64, 128, 192], [0.5, 0.3, 0.2])\n    ]\n    \n    \n    # ============================================================\n    # === Build model and global Warmup + Cosine LR schedule ===\n    # ============================================================\n    # with strategy.scope():\n    model = build_model()\n    EPOCHS = 12\n    \n    def calculate_stage_steps_per_epoch(seqs, weights):\n        \"\"\"Calculates the weighted steps_per_epoch for a single stage.\"\"\"\n        # This calculation is correct based on your existing code logic:\n        return int(sum(train_steps[s] * w for s, w in zip(seqs, weights)))\n    \n    class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n        def __init__(self, base_lr, total_steps, warmup_steps, min_lr=1e-6):\n            super().__init__()\n            self.base_lr = base_lr\n            self.warmup_steps = warmup_steps\n            self.total_steps = total_steps\n            self.min_lr = min_lr\n    \n        def __call__(self, step):\n            # Linear warmup\n            warmup_lr = self.base_lr * (tf.cast(step, tf.float32) / tf.cast(self.warmup_steps, tf.float32))\n            \n            # Cosine decay after warmup\n            progress = (tf.cast(step - self.warmup_steps, tf.float32) /\n                        tf.cast(self.total_steps - self.warmup_steps, tf.float32))\n            cosine_decay = 0.5 * (1 + tf.cos(np.pi * tf.clip_by_value(progress, 0.0, 1.0)))\n            cosine_lr = self.min_lr + (self.base_lr - self.min_lr) * cosine_decay\n            \n            return tf.cond(step < self.warmup_steps, lambda: warmup_lr, lambda: cosine_lr)\n            \n        def get_config(self): # <--- THIS IS THE REQUIRED FIX\n            \"\"\"Returns the serializable configuration of the schedule.\"\"\"\n            return {\n                \"base_lr\": self.base_lr,\n                \"total_steps\": self.total_steps,\n                \"warmup_steps\": self.warmup_steps,\n                \"min_lr\": self.min_lr,\n            }\n    \n        # Optional: Keras automatically handles from_config if get_config returns\n        # arguments matching the __init__ signature, but defining it is safer.\n        @classmethod\n        def from_config(cls, config):\n            return cls(**config)\n    \n    def get_lr_schedule(stage_steps):\n        \"\"\"Create a new Warmup + Cosine Decay schedule per stage.\"\"\"\n        warmup_steps = int(0.1 * stage_steps)\n        # warmup_steps = 4000\n        return WarmUpCosine(\n            base_lr=1e-3,\n            total_steps=stage_steps,\n            warmup_steps=warmup_steps,\n            min_lr=1e-6\n        )\n    \n    if USE_ABLATION:\n        print(\"ðŸ§ª Ablation mode active: static training setup\")\n        total_steps = EPOCHS * sum(1 for _ in datasets_by_size[\"train\"])\n        lr_schedule = get_lr_schedule(total_steps)\n        model.compile(optimizer=mixed_precision.LossScaleOptimizer(tf.keras.optimizers.AdamW(lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9, clipnorm=1.0)), loss=\"huber\", metrics=[\"mae\"])\n    else:\n        x,y,z = sum(1 for _ in datasets_by_size[64]['train']),sum(1 for _ in datasets_by_size[128]['train']),sum(1 for _ in datasets_by_size[192]['train'])\n        # x1,y1,z1 = sum(1 for _ in datasets_by_size[64]['val']),sum(1 for _ in datasets_by_size[128]['val']),sum(1 for _ in datasets_by_size[192]['val'])\n        train_steps = {64: x, 128: y, 192: z}\n        # val_steps = {64: x1, 128: y1, 192: z1}\n        # Calculate the steps for all three stages:\n        total_calculated_steps = 0\n        for seq_sizes, weights in curriculum_stages:\n            stage_steps_per_epoch = calculate_stage_steps_per_epoch(seq_sizes, weights)\n            total_calculated_steps += stage_steps_per_epoch * EPOCHS\n        print(\"ðŸŽ“ Curriculum Learning mode active\")\n        lr_schedule = get_lr_schedule(total_calculated_steps)\n        optimizer = mixed_precision.LossScaleOptimizer(tf.keras.optimizers.AdamW(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9, clipnorm=1.0))\n        model.compile(optimizer=optimizer, loss=\"huber\", metrics=[\"mae\"])\n    \n    # print(\"trainable_weights\",sum(w.numpy().size for w in model.trainable_weights))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating pre-trained model checkpoint\nMake sure if training discontinues, then it resumes from the last checkpoint instead of starting from epoch 1. It will only work if the checkpoint is downloaded from 'kaggle/working/bert' and then uploaded to 'kaggle/input/bert-1'.","metadata":{}},{"cell_type":"code","source":"if USE_FINE_TUNE==False:\n    from tensorflow.keras.callbacks import Callback\n    # Required custom objects for model/optimizer deserialization\n    custom_objects = {\n        \"PositionalEmbedding\": PositionalEmbedding,\n        \"WarmUpCosine\": WarmUpCosine,\n        \"padfloat_to_attnbool\": padfloat_to_attnbool,\n        \"LossScaleOptimizer\": tf.keras.mixed_precision.LossScaleOptimizer,\n    }\n    \n    class TimeBasedEpochCheckpoint(callbacks.Callback):\n        \"\"\"\n        Saves at epoch end (atomic), writes metadata (.meta.json), and keeps only the latest `last_n_checkpoints`.\n        Robust pruning: handles checkpoint artifacts that may be directories OR single files.\n        \"\"\"\n        def __init__(self, basepath, interval_minutes=0.0, last_n_checkpoints=5, verbose=1):\n            super().__init__()\n            self.basepath = basepath\n            self.base_dir = os.path.dirname(basepath) or \".\"\n            os.makedirs(self.base_dir, exist_ok=True)\n            self.interval_seconds = float(interval_minutes) * 60.0\n            self.last_n_checkpoints = int(last_n_checkpoints)\n            self.verbose = int(verbose)\n            self.last_save_time = None\n            self.stage_idx = 0\n            self.base_basename = os.path.basename(basepath)\n    \n        def set_stage(self, stage_idx: int):\n            self.stage_idx = int(stage_idx)\n    \n        def on_train_begin(self, logs=None):\n            self.last_save_time = time.time() - (self.interval_seconds + 1.0)\n    \n        def on_epoch_end(self, epoch, logs=None):\n            now = time.time()\n            if self.interval_seconds > 0 and (now - (self.last_save_time or 0.0)) < self.interval_seconds:\n                if self.verbose:\n                    print(f\"[TimeBasedEpochCheckpoint] skipping save at epoch {epoch} (interval not reached)\")\n                return\n    \n            ts = int(now)\n            ckpt_name = f\"{self.base_basename}_stage{self.stage_idx}_e{epoch}_t{ts}.keras\"\n            ckpt_path_target = os.path.join(self.base_dir, ckpt_name)\n            meta_path = ckpt_path_target + \".meta.json\"\n    \n            tmp_dir = None\n            try:\n                tmp_dir = tempfile.mkdtemp(dir=self.base_dir)\n                tmp_ckpt_path = os.path.join(tmp_dir, \"model.keras\")  # could be file or dir after save\n                if self.verbose:\n                    print(f\"\\n[TimeBasedEpochCheckpoint] Saving model to temporary path {tmp_ckpt_path} ...\")\n    \n                # Save model including optimizer state\n                # model.save may create either a directory or a single file at tmp_ckpt_path\n                self.model.save(tmp_ckpt_path, overwrite=True, include_optimizer=True)\n    \n                # read optimizer.iterations for metadata\n                global_step = None\n                try:\n                    global_step = int(self.model.optimizer.iterations.numpy())\n                except Exception:\n                    try:\n                        global_step = int(tf.keras.backend.get_value(self.model.optimizer.iterations))\n                    except Exception:\n                        global_step = None\n    \n                metadata = {\n                    \"stage\": int(self.stage_idx),\n                    \"epoch\": int(epoch),\n                    \"global_step\": global_step,\n                    \"timestamp\": ts,\n                    \"ckpt_path\": ckpt_path_target\n                }\n                tmp_meta_path = os.path.join(tmp_dir, \"meta.json\")\n                with open(tmp_meta_path, \"w\") as f:\n                    json.dump(metadata, f, indent=2)\n    \n                # Remove existing target if present (handle file/dir)\n                if os.path.exists(ckpt_path_target):\n                    try:\n                        if os.path.isdir(ckpt_path_target):\n                            shutil.rmtree(ckpt_path_target)\n                        else:\n                            os.remove(ckpt_path_target)\n                    except Exception as e:\n                        if self.verbose:\n                            print(f\"[TimeBasedEpochCheckpoint] Warning removing existing target {ckpt_path_target}: {e}\")\n    \n                # Move saved artifact into final place. Works whether tmp_ckpt_path is file or dir.\n                shutil.move(tmp_ckpt_path, ckpt_path_target)\n                shutil.move(tmp_meta_path, meta_path)\n    \n                self.last_save_time = now\n                if self.verbose:\n                    print(f\"[TimeBasedEpochCheckpoint] Saved checkpoint {ckpt_path_target} (stage={self.stage_idx}, epoch={epoch}, global_step={global_step})\")\n                # prune older checkpoints\n                self._prune_old_checkpoints()\n            finally:\n                # best-effort cleanup of tmp_dir\n                try:\n                    if tmp_dir and os.path.exists(tmp_dir):\n                        shutil.rmtree(tmp_dir)\n                except Exception:\n                    pass\n    \n        def _prune_old_checkpoints(self):\n            \"\"\"\n            Find matching metadata files and remove the oldest checkpoint artifacts until only\n            `last_n_checkpoints` remain. Each old ckpt_path may be a directory or a file.\n            \"\"\"\n            pattern = os.path.join(self.base_dir, f\"{self.base_basename}_stage*_e*_t*.keras.meta.json\")\n            metas = glob.glob(pattern)\n            if len(metas) <= self.last_n_checkpoints:\n                return\n    \n            # sort by modification time (oldest first)\n            metas_sorted = sorted(metas, key=os.path.getmtime)\n            num_to_remove = len(metas_sorted) - self.last_n_checkpoints\n            to_remove = metas_sorted[:num_to_remove]\n    \n            for meta_path in to_remove:\n                try:\n                    with open(meta_path, \"r\") as f:\n                        meta_obj = json.load(f)\n                    ckpt_path = meta_obj.get(\"ckpt_path\") or meta_obj.get(\"ckpt_dir\")  # support older field name\n                    if ckpt_path:\n                        if os.path.exists(ckpt_path):\n                            try:\n                                if os.path.isdir(ckpt_path):\n                                    shutil.rmtree(ckpt_path)\n                                    if self.verbose:\n                                        print(f\"[TimeBasedEpochCheckpoint] Removed old checkpoint directory: {ckpt_path}\")\n                                elif os.path.isfile(ckpt_path):\n                                    os.remove(ckpt_path)\n                                    if self.verbose:\n                                        print(f\"[TimeBasedEpochCheckpoint] Removed old checkpoint file: {ckpt_path}\")\n                                else:\n                                    # could be special file/symlink -> try remove\n                                    try:\n                                        os.remove(ckpt_path)\n                                        if self.verbose:\n                                            print(f\"[TimeBasedEpochCheckpoint] Removed old checkpoint (unknown type): {ckpt_path}\")\n                                    except Exception:\n                                        if self.verbose:\n                                            print(f\"[TimeBasedEpochCheckpoint] Could not remove unknown checkpoint path: {ckpt_path}\")\n                            except Exception as e:\n                                if self.verbose:\n                                    print(f\"[TimeBasedEpochCheckpoint] Warning while removing {ckpt_path}: {e}\")\n                        else:\n                            if self.verbose:\n                                print(f\"[TimeBasedEpochCheckpoint] Old checkpoint path not found (already removed?): {ckpt_path}\")\n                    # remove meta file\n                    try:\n                        os.remove(meta_path)\n                    except Exception as e:\n                        if self.verbose:\n                            print(f\"[TimeBasedEpochCheckpoint] Warning removing meta file {meta_path}: {e}\")\n                except Exception as e:\n                    if self.verbose:\n                        print(f\"[TimeBasedEpochCheckpoint] Warning while pruning {meta_path}: {e}\")\n    \n    # Initialize the epoch checkpoint callback\n    CHECKPOINT_BASE = \"/kaggle/working/bert\"  # final checkpoint dirs will be like /kaggle/working/bert_stage{...}.keras\n    time_ckpt = TimeBasedEpochCheckpoint(\n        basepath=CHECKPOINT_BASE,\n        interval_minutes=0.0,   # 0.0 => save at every epoch end\n        last_n_checkpoints=1,\n        verbose=1\n    )\n    \n    def find_latest_meta(base_dir, basename):\n        pattern = os.path.join(base_dir, f\"{basename}_stage*_e*_t*.keras.meta.json\")\n        metas = sorted(glob.glob(pattern), key=os.path.getmtime, reverse=True)\n        return metas[0] if metas else None\n    \n    def load_checkpoint_from_meta(meta_path):\n        \"\"\"Return tuple (model, meta_dict) or raise.\"\"\"\n        with open(meta_path, \"r\") as f:\n            meta = json.load(f)\n    \n        ckpt_path = meta.get(\"ckpt_path\") or meta.get(\"ckpt_dir\")\n        if not ckpt_path:\n            raise FileNotFoundError(\"metadata missing ckpt_path\")\n    \n        # NEW: Rewrite path so it loads from /kaggle/input instead of /kaggle/working\n        ckpt_file = os.path.basename(ckpt_path)  # \"bert_stage2_e0_tXXXX.keras\"\n        ckpt_path = os.path.join(os.path.dirname(meta_path), ckpt_file)\n    \n        print(\"[resume] rewritten ckpt_path =\", ckpt_path)\n    \n        if not os.path.exists(ckpt_path):\n            raise FileNotFoundError(f\"rewritten ckpt_path does not exist: {ckpt_path}\")\n    \n        # Try loading including optimizer\n        try:\n            print(f\"[resume] trying tf.keras.models.load_model({ckpt_path})\")\n            m = tf.keras.models.load_model(\n                ckpt_path, custom_objects=custom_objects, compile=True\n            )\n            print(\"[resume] load_model succeeded (full model + optimizer restored)\")\n            return m, meta\n    \n        except Exception as e:\n            print(f\"[resume] load_model failed: {e}. Falling back to build_model() + load_weights()\")\n            m = build_model()\n            m.load_weights(ckpt_path)\n            return m, meta\n    \n    \n    \n    \n    if USE_ABLATION:\n            train_history = {\"loss\": [], \"val_loss\": [],\"mae\":[],\"val_mae\":[]}\n            s = time.time()\n            es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n            # reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n            history = model.fit(\n                datasets_by_size[\"train\"],\n                validation_data=datasets_by_size[\"val\"],\n                epochs=EPOCHS,\n                callbacks=[es],\n                verbose=1\n            )\n    \n            train_history[\"loss\"].extend(history.history[\"loss\"])\n            train_history[\"val_loss\"].extend(history.history[\"val_loss\"])\n            train_history[\"mae\"].extend(history.history[\"mae\"])\n            train_history[\"val_mae\"].extend(history.history[\"val_mae\"])\n            all_histories['GeLU'] = history.history\n            e = time.time()\n            print(f\"tfrecord finished. Elapsed time: {(e-s)/60:.2f} minutes\")\n        \n    else:\n        # ---------------------------\n        # Attempt to resume from last checkpoint (if any)\n        # ---------------------------\n        BASE_DIR = \"/kaggle/input/bert-1\"      # (same variable you use later) \n        BASENAME = \"bert\"\n        latest_meta = find_latest_meta(BASE_DIR, BASENAME)\n        \n        resume_info = None\n        if latest_meta is not None:\n            print(\"[resume] Found latest meta:\", latest_meta)\n            try:\n                model, meta = load_checkpoint_from_meta(latest_meta)\n                # ensure model is compiled (if load_model returned compile=True it is compiled)\n                # If not compiled, compile with the same optimizer configuration used originally.\n                if model.optimizer is None:\n                    print(\"[resume] model was not compiled; compiling manually\")\n                    lr_sched = get_lr_schedule(total_calculated_steps)\n                    opt = tf.keras.optimizers.AdamW(\n                        learning_rate=lr_sched,\n                        beta_1=0.9, beta_2=0.98,\n                        epsilon=1e-9,\n                        clipnorm=1.0\n                    )\n                    policy = mixed_precision.global_policy()\n                    policy_name = policy.name if hasattr(policy, \"name\") else str(policy)\n                    \n                    if policy_name.startswith(\"mixed\"):\n                        opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n    \n                \n                    model.compile(optimizer=opt, loss=\"huber\", metrics=[\"mae\"])\n                else:\n                    print(\"[resume] model already compiled (optimizer restored)\")\n                # set optimizer.iterations to saved global_step if present (ensures LR schedule will resume)\n                try:\n                    gs = meta.get(\"global_step\")\n                    if gs is not None:\n                        target_step = int(gs)\n                        # 1. Check current value first (load_model usually restores this automatically)\n                        current_step = int(model.optimizer.iterations.numpy())\n                        \n                        if current_step == target_step:\n                            print(f\"[resume] optimizer.iterations is already {current_step}. No action needed.\")\n                        else:\n                            print(f\"[resume] Adjusting optimizer.iterations from {current_step} to {target_step}\")\n                            # 2. Use .assign() directly on the variable, bypassing backend.set_value\n                            model.optimizer.iterations.assign(target_step)\n                            \n                except Exception as e:\n                    print(f\"[resume] Warning: Could not manually set optimizer iterations: {e}\")\n                    # This is non-fatal if load_model succeeded\n        \n                # read saved stage & epoch\n                saved_stage = int(meta.get(\"stage\", 1))\n                saved_epoch = int(meta.get(\"epoch\", 0))\n                resume_info = {\"stage\": saved_stage, \"epoch\": saved_epoch, \"meta\": meta}\n                print(f\"[resume] Resume prepared: stage={saved_stage}, epoch={saved_epoch}\")\n            except Exception as e:\n                print(\"[resume] resume attempt failed:\", e)\n                resume_info = None\n        else:\n            print(\"[resume] No previous checkpoint found. Starting fresh.\")\n    \n        # Training loop per stage (reusing model weights)\n        time_ckpt.on_train_begin()\n        for stage_idx, (seqs, weights) in enumerate(curriculum_stages, 1):\n            # e=time.time()\n            print(f\"\\nStage {stage_idx} sequences={seqs} weights={weights}\")\n            time_ckpt.set_stage(stage_idx)\n            # if resuming from a later stage, skip earlier stages\n            if resume_info is not None and resume_info[\"stage\"] > stage_idx:\n                print(f\"[resume] skipping stage {stage_idx} (already completed)\")\n                continue\n            merged_train = tf.data.Dataset.sample_from_datasets(\n            [datasets_by_size[s][\"train\"] for s in seqs], weights, seed=42, stop_on_empty_dataset=True\n            )\n            # merged_val = tf.data.Dataset.sample_from_datasets(\n            #     [datasets_by_size[s][\"val\"] for s in seqs], weights, seed=42, stop_on_empty_dataset=True\n            # )\n            merged_train = merged_train.repeat()\n            # merged_val = merged_val.repeat()\n            stage_train_steps = int(sum(train_steps[s] * w for s, w in zip(seqs, weights)))  # weighted sum\n            # stage_val_steps   = int(sum(val_steps[s] * w for s, w in zip(seqs, weights)))\n            \n            # determine initial_epoch for this stage\n            if resume_info is not None and resume_info[\"stage\"] == stage_idx:\n                initial_epoch = int(resume_info[\"epoch\"]) + 1   # start from next epoch\n                print(f\"[resume] Resuming stage {stage_idx} from epoch {initial_epoch}\")\n            else:\n                initial_epoch = 0\n            \n            history = model.fit(\n                merged_train,\n                epochs=EPOCHS,\n                initial_epoch=initial_epoch,\n                steps_per_epoch=stage_train_steps,\n                callbacks=[time_ckpt],\n                verbose=1\n            )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## (optional) aggregating weights from the last three epochs of the pre-training \ncommon practice in LLM/MLM to make model generalise better, however we did not observed any imrovement.  ","metadata":{}},{"cell_type":"code","source":"if USE_FINE_TUNE==False and USE_ABLATION==False:\n    # Parameters: adapt as needed\n    BASE_DIR = \"/kaggle/working\"         # directory where checkpoints/meta JSONs live\n    BASENAME = \"bert\"                   # base used in TimeBasedEpochCheckpoint basepath (e.g. bert -> bert_stage*.keras and bert_*.meta.json)\n    N_LAST = 3                          # number of last checkpoints to average\n    OUTPATH = \"/kaggle/working/BERT_Final_Averaged_Model.keras\"  # final save path\n    \n    custom_objects = {\n        \"PositionalEmbedding\": PositionalEmbedding,\n        \"WarmUpCosine\": WarmUpCosine,\n        \"padfloat_to_attnbool\": padfloat_to_attnbool,\n        \"LossScaleOptimizer\": tf.keras.mixed_precision.LossScaleOptimizer,\n    }\n    \n    # 1) Find last N meta files (most recent modification time)\n    pattern = os.path.join(BASE_DIR, f\"{BASENAME}_stage*_e*_t*.keras.meta.json\")\n    meta_paths = sorted(glob.glob(pattern), key=os.path.getmtime, reverse=True)\n    meta_paths = meta_paths[:N_LAST]\n    \n    if not meta_paths:\n        raise RuntimeError(f\"No checkpoint metadata found with pattern {pattern}\")\n    \n    print(\"Averaging these checkpoint metas (newest first):\")\n    for p in meta_paths:\n        print(\" \", p)\n    \n    # 2) Load weights and accumulate\n    accum_weights = None\n    count = 0\n    for meta_path in meta_paths:\n        with open(meta_path, \"r\") as f:\n            meta = json.load(f)\n        ckpt_path = meta.get(\"ckpt_path\") or meta.get(\"ckpt_dir\")  # support variations\n        if not ckpt_path:\n            print(f\"Skipping meta {meta_path}: no ckpt_path found\")\n            continue\n        if not os.path.exists(ckpt_path):\n            print(f\"Skipping meta {meta_path}: ckpt path does not exist: {ckpt_path}\")\n            continue\n    \n        print(f\"Loading checkpoint: {ckpt_path}\")\n        # load saved model (weights + architecture); use compile=False to avoid loading optimizer\n        try:\n            m = tf.keras.models.load_model(ckpt_path, custom_objects=custom_objects, compile=False)\n        except Exception as e:\n            # fallback: if the saved artifact contains only weights in a single file, try load_weights on fresh model\n            print(f\"load_model failed for {ckpt_path} with error: {e}\")\n            print(\"Attempting to load weights via build_model() + load_weights(...)\")\n            m = build_model()\n            m.load_weights(ckpt_path)  # may still fail; let exception bubble if so\n    \n        w = m.get_weights()\n        # convert to numpy arrays with a consistent dtype (float64 accumulation)\n        w = [x.astype(np.float64) for x in w]\n    \n        if accum_weights is None:\n            accum_weights = [np.zeros_like(x, dtype=np.float64) for x in w]\n    \n        if len(w) != len(accum_weights):\n            raise RuntimeError(f\"Weight length mismatch for checkpoint {ckpt_path}: expected {len(accum_weights)} arrays, got {len(w)}\")\n    \n        for i in range(len(w)):\n            accum_weights[i] += w[i]\n    \n        count += 1\n    \n    if count == 0:\n        raise RuntimeError(\"No valid checkpoints were loaded for averaging\")\n    \n    # 3) Compute average and cast back to float32\n    avg_weights = [ (w / float(count)).astype(np.float32) for w in accum_weights ]\n    \n    # 4) Build final model and set averaged weights\n    final_model = build_model()   # ensure this uses same args as original model\n    final_model.set_weights(avg_weights)\n    \n    # 5) Save final averaged model (SavedModel / .keras format)\n    final_model.save(OUTPATH, overwrite=True)\n    print(f\"Final averaged model saved to: {OUTPATH} (averaged {count} checkpoints)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plotting ablation result \nmake sure variable 'USE_ABLATION' was set to True ","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# Plot train/val loss curves\n# --------------------------\nif USE_FINE_TUNE==False and USE_ABLATION==True:\n    def plot_ablation_histories(all_histories, metric=\"loss\", epoch_limit=None, figsize=(8,5)):\n        \"\"\"\n        all_histories: dict mapping seq_len -> history dict (history['loss'] and history['val_loss'] present)\n        metric: \"loss\" or \"mae\" (plots metric and 'val_' + metric)\n        epoch_limit: optional int to truncate x-axis\n        \"\"\"\n        seqs = sorted(all_histories.keys())\n        # pick a color cycle with enough distinct colors\n        cmap = plt.get_cmap(\"tab10\")\n        colors = [cmap(i % 10) for i in range(len(seqs))]\n    \n        # determine longest epoch length across histories\n        max_epochs = max(len(all_histories[s][metric]) for s in seqs)\n        if epoch_limit is not None:\n            max_epochs = min(max_epochs, epoch_limit)\n        x = np.arange(1, max_epochs + 1)\n    \n        plt.figure(figsize=figsize)\n        band_patches = []  # for legend entries of bands\n    \n        for i, seq in enumerate(seqs):\n            hist = all_histories[seq]\n            train = np.array(hist[metric])\n            val = np.array(hist[f\"val_{metric}\"])\n    \n            # pad to max_epochs by repeating last value if needed\n            if train.size < max_epochs:\n                train = np.pad(train, (0, max_epochs - train.size), mode='edge')\n            if val.size < max_epochs:\n                val = np.pad(val, (0, max_epochs - val.size), mode='edge')\n    \n            # optionally truncate\n            train = train[:max_epochs]\n            val = val[:max_epochs]\n    \n            # lower and upper bounds for the band\n            lower = np.minimum(train, val)\n            upper = np.maximum(train, val)\n    \n            color = colors[i]\n            label_train = f\"Train ({seq})\"\n            label_val   = f\"Val ({seq})\"\n    \n            # fill band between train and val\n            plt.fill_between(x, lower, upper, color=color, alpha=0.12, linewidth=0, zorder=1)\n    \n            # plot the lines on top (train solid, val dashed)\n            plt.plot(x, train, linestyle='-', linewidth=1.5, color=color, alpha=0.9, zorder=3)\n            plt.plot(x, val,   linestyle='--', linewidth=1.2, color=color, alpha=0.9, zorder=4)\n    \n            # small legend patch for band only (optional)\n            band_patches.append(Patch(facecolor=color, alpha=0.12, label=f\"Loss = {seq}\"))\n        if metric==\"mae\":\n            plt.title(f\"Train & Validation MAE â€” Loss function ablation comparison\")\n        else:\n            plt.title(f\"Train & Validation {metric.capitalize()} â€” Loss function ablation comparison\")\n        plt.xlabel(\"Epoch\",fontsize=13)\n        if metric==\"mae\":\n            plt.ylabel(\"MAE\",fontsize=13)\n        else:\n            plt.ylabel(metric.capitalize(),fontsize=13)\n        plt.grid(True, linestyle=\":\", alpha=0.5)\n    \n        # construct legends:\n        #  - one for bands describing sequence lengths\n        #  - one for line styles (train/val)\n        # proxies for line styles\n        from matplotlib.lines import Line2D\n        line_proxies = [\n            Line2D([0], [0], color='k', linestyle='-', linewidth=1.5, label='Train'),\n            Line2D([0], [0], color='k', linestyle='--', linewidth=1.2, label='Val')\n        ]\n    \n        # top-left: style legend; top-right: seq legend (bands)\n        l1 = plt.legend(handles=line_proxies, loc='upper left')\n        l2 = plt.legend(handles=band_patches, loc='upper right', ncol=1, framealpha=0.9)\n        plt.gca().add_artist(l1)  # keep first legend\n        plt.tight_layout()\n        plt.show()\n        \n    plot_ablation_histories(all_histories, metric=\"loss\", epoch_limit=EPOCHS)\n    plot_ablation_histories(all_histories, metric=\"mae\",  epoch_limit=EPOCHS)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-tuning module\nMake sure variable 'USE_FINE_TUNE' is set to True","metadata":{}},{"cell_type":"code","source":"# test\nif USE_FINE_TUNE:\n    tf.keras.mixed_precision.set_global_policy('float32')\n    # --------------------------------\n    # 2. LORA IMPLEMENTATION\n    # --------------------------------\n    class LoRALayer(tf.keras.layers.Wrapper):\n        def __init__(self, layer, rank=4, alpha=4, **kwargs):\n            super().__init__(layer, **kwargs)\n            self.rank = int(rank)\n            self.alpha = float(alpha)\n            self.scaling = None\n    \n        def build(self, input_shape):\n            in_dim = int(input_shape[-1])\n            out_dim = self.layer.units \n            \n            self.lora_A = self.add_weight(\n                name=self.layer.name + \"_lora_A\",\n                shape=(in_dim, self.rank),\n                initializer=\"random_normal\",\n                trainable=True,\n                dtype=\"float32\"\n            )\n            self.lora_B = self.add_weight(\n                name=self.layer.name + \"_lora_B\",\n                shape=(self.rank, out_dim),\n                initializer=\"zeros\",\n                trainable=True,\n                dtype=\"float32\"\n            )\n            self.scaling = self.alpha / self.rank\n            super().build(input_shape)\n    \n        def call(self, inputs, **kwargs):\n            x_32 = tf.cast(inputs, tf.float32)\n            base_out = self.layer(x_32, **kwargs)\n            lora_out = tf.matmul(x_32, self.lora_A)\n            lora_out = tf.matmul(lora_out, self.lora_B) * self.scaling\n            return tf.cast(base_out, tf.float32) + lora_out\n            \n        def get_config(self):\n            config = super().get_config()\n            config.update({\"rank\": self.rank, \"alpha\": self.alpha})\n            return config\n    \n    def add_lora_to_encoder(encoder, rank=4, alpha=8):\n        def replace_dense(layer):\n            # A. Replace Dense with LoRA\n            if isinstance(layer, tf.keras.layers.Dense):\n                cfg = layer.get_config()\n                cfg.pop('dtype', None); cfg.pop('dtype_policy', None); cfg['dtype'] = 'float32'\n                new_dense = tf.keras.layers.Dense.from_config(cfg)\n                return LoRALayer(new_dense, rank=rank, alpha=alpha)\n            # B. Keep other layers (Prevents NoneType error on Lambda/Custom)\n            return layer\n    \n        new_encoder = tf.keras.models.clone_model(encoder, clone_function=replace_dense)\n        \n        print(\"Copying weights...\")\n        for old_lyr, new_lyr in zip(encoder.layers, new_encoder.layers):\n            if isinstance(new_lyr, LoRALayer):\n                new_lyr.layer.set_weights(old_lyr.get_weights())\n    \n        print(\"Freezing base layers...\")\n        for layer in new_encoder.layers:\n            layer.trainable = False\n            if isinstance(layer, LoRALayer):\n                layer.trainable = True\n                layer.layer.trainable = False\n        return new_encoder\n    \n    # --------------------------------\n    # 3. DATASET & TRAINING SETUP\n    # --------------------------------\n    def parse_record(example):\n        feature_description = {\n            \"x\": tf.io.FixedLenFeature([fine_tune_seq_len * NUM_FEATURES], tf.float32),\n            \"y\": tf.io.FixedLenFeature([fine_tune_seq_len * NUM_LABELS], tf.float32),\n            \"mask\": tf.io.FixedLenFeature([fine_tune_seq_len], tf.float32),\n        }\n        ex = tf.io.parse_single_example(example, feature_description)\n        x = tf.reshape(ex[\"x\"], (fine_tune_seq_len, NUM_FEATURES))\n        y = tf.reshape(ex[\"y\"], (fine_tune_seq_len, NUM_LABELS))\n        mask = tf.reshape(ex[\"mask\"], (fine_tune_seq_len,))\n        return (x, mask), y, mask\n    \n    def make_dataset(path, batch_size, training,drop_remainder):\n        ds = tf.data.TFRecordDataset(path, num_parallel_reads=AUTOTUNE)\n        ds = ds.map(parse_record, num_parallel_calls=AUTOTUNE)\n        if training:\n            ds = ds.shuffle(2000, seed=42)\n        return ds.batch(batch_size, drop_remainder=drop_remainder).prefetch(AUTOTUNE)\n\n\n    def parse_record2(example):\n        \"\"\"\n        Parses a single TFRecord example, now including doc_id and veh_id.\n        \"\"\"\n        feature_description = {\n            \"x\": tf.io.FixedLenFeature([fine_tune_seq_len * NUM_FEATURES], tf.float32),\n            \"y\": tf.io.FixedLenFeature([fine_tune_seq_len * NUM_LABELS], tf.float32),\n            \"mask\": tf.io.FixedLenFeature([fine_tune_seq_len], tf.float32),\n            # NEW: Features for the string IDs (stored as bytes in the TFRecord)\n            \"doc_id\": tf.io.FixedLenFeature([], tf.string),\n            \"veh_id\": tf.io.FixedLenFeature([], tf.string),\n        }\n        \n        ex = tf.io.parse_single_example(example, feature_description)\n        \n        x = tf.reshape(ex[\"x\"], (fine_tune_seq_len, NUM_FEATURES))\n        y = tf.reshape(ex[\"y\"], (fine_tune_seq_len, NUM_LABELS))\n        mask = tf.reshape(ex[\"mask\"], (fine_tune_seq_len,))\n        \n        doc_id = ex[\"doc_id\"]\n        veh_id = ex[\"veh_id\"]\n        \n        # Return 5 items: ((features), labels, sample_weight, ID1, ID2)\n        return (x, mask), y, mask, doc_id, veh_id\n    \n    def make_dataset2(path, batch_size, training, drop_remainder):\n        \"\"\"Creates a TF Dataset from TFRecord paths.\"\"\"\n        ds = tf.data.TFRecordDataset(path, num_parallel_reads=AUTOTUNE)\n        # The map function now returns 5 elements\n        ds = ds.map(parse_record2, num_parallel_calls=AUTOTUNE)\n        return ds.batch(batch_size, drop_remainder=drop_remainder).prefetch(AUTOTUNE)\n\n    \n    \n    def build_finetune_model(encoder):\n        inp = tf.keras.Input(shape=(fine_tune_seq_len, NUM_FEATURES), dtype=tf.float32)\n        mask = tf.keras.Input(shape=(fine_tune_seq_len,), dtype=tf.float32)\n        x = encoder([inp, mask])\n        out = tf.keras.layers.Dense(NUM_LABELS, activation=\"sigmoid\", dtype=\"float32\")(x)\n        return tf.keras.Model([inp, mask], out)\n\n    # [Keep your WarmUpCosine, PositionalEmbedding, padfloat_to_attnbool, custom_objects here]\n\n    class PositionalEmbedding(tf.keras.layers.Layer):\n        def __init__(self, max_seq_len, d_model, **kwargs):\n            super().__init__(**kwargs)\n            self.max_seq_len = max_seq_len\n            self.d_model = d_model\n            self.pos_emb = tf.keras.layers.Embedding(\n                input_dim=max_seq_len,\n                output_dim=d_model,\n                dtype=\"float32\",\n                name=\"pos_embedding\"\n            )\n    \n        def build(self, input_shape):\n            # Explicitly build the internal Embedding layer\n            self.pos_emb.build((None,))\n            super().build(input_shape)\n    \n        def call(self, x):\n            seq_len = tf.shape(x)[1]\n            positions = tf.range(seq_len)\n            pos_embeddings = self.pos_emb(positions)\n            pos_embeddings = tf.cast(pos_embeddings, x.dtype)\n            return x + pos_embeddings\n    \n        def get_config(self):\n            config = super().get_config()\n            config.update({\n                \"max_seq_len\": self.max_seq_len,\n                \"d_model\": self.d_model,\n            })\n            return config\n\n\n\n    \n    def padfloat_to_attnbool(m):\n        return tf.expand_dims(tf.expand_dims(tf.cast(m > 0.5, tf.bool), 1), 1)\n\n    class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n        def __init__(self, base_lr, total_steps, warmup_steps, min_lr=1e-6):\n            super().__init__()\n            self.base_lr = base_lr\n            self.warmup_steps = warmup_steps\n            self.total_steps = total_steps\n            self.min_lr = min_lr\n    \n        def __call__(self, step):\n            # Linear warmup\n            warmup_lr = self.base_lr * (tf.cast(step, tf.float32) / tf.cast(self.warmup_steps, tf.float32))\n            \n            # Cosine decay after warmup\n            progress = (tf.cast(step - self.warmup_steps, tf.float32) /\n                        tf.cast(self.total_steps - self.warmup_steps, tf.float32))\n            cosine_decay = 0.5 * (1 + tf.cos(np.pi * tf.clip_by_value(progress, 0.0, 1.0)))\n            cosine_lr = self.min_lr + (self.base_lr - self.min_lr) * cosine_decay\n            \n            return tf.cond(step < self.warmup_steps, lambda: warmup_lr, lambda: cosine_lr)\n            \n        def get_config(self): # <--- THIS IS THE REQUIRED FIX\n            \"\"\"Returns the serializable configuration of the schedule.\"\"\"\n            return {\n                \"base_lr\": self.base_lr,\n                \"total_steps\": self.total_steps,\n                \"warmup_steps\": self.warmup_steps,\n                \"min_lr\": self.min_lr,\n            }\n    \n        # Optional: Keras automatically handles from_config if get_config returns\n        # arguments matching the __init__ signature, but defining it is safer.\n        @classmethod\n        def from_config(cls, config):\n            return cls(**config)\n    \n    def get_lr_schedule(stage_steps):\n        \"\"\"Create a new Warmup + Cosine Decay schedule per stage.\"\"\"\n        warmup_steps = int(0.01 * stage_steps)\n        return WarmUpCosine(\n            base_lr=1e-3,\n            total_steps=stage_steps,\n            warmup_steps=warmup_steps,\n            min_lr=1e-6\n        )\n\n\n    class MaskedBinaryCrossentropy(tf.keras.losses.Loss):\n        def __init__(self, from_logits=False, name=\"masked_bce\"):\n            super().__init__(name=name)\n            self.from_logits = from_logits\n            self.bce = tf.keras.losses.BinaryCrossentropy(\n                from_logits=from_logits,\n                reduction=tf.keras.losses.Reduction.NONE\n            )\n    \n        def call(self, y_true, y_pred, sample_weight=None):\n            \"\"\"\n            y_true: (B, T, L)\n            y_pred: (B, T, L)\n            sample_weight (mask): (B, T)\n            \"\"\"\n            # Per-label BCE â†’ (B, T, L)\n            loss = self.bce(y_true, y_pred)\n    \n            # Reduce labels â†’ (B, T)\n            loss = tf.reduce_mean(loss, axis=-1)\n    \n            if sample_weight is not None:\n                sample_weight = tf.cast(sample_weight, loss.dtype)\n                loss = loss * sample_weight\n                return tf.reduce_sum(loss) / (tf.reduce_sum(sample_weight) + 1e-8)\n    \n            # Fallback (should not happen)\n            return tf.reduce_mean(loss)\n\n\n    #-------------- F1 Score evaluation-------#\n    def collect_predictions(model, dataset):\n        \"\"\"\n        Collects predictions from the model, now also collecting document and vehicle IDs.\n        \"\"\"\n        y_true_all = []\n        y_pred_all = []\n        mask_all = []\n        doc_id_all = [] # New list for Document IDs\n        veh_id_all = [] # New list for Vehicle IDs\n    \n        # NOTE: The loop now unpacks 5 values from the dataset element\n        for (x, attn_mask), y, valid_mask, doc_id, veh_id in dataset:\n            y_pred = model.predict_on_batch((x, attn_mask))\n            \n            y_true_all.append(y.numpy())\n            y_pred_all.append(y_pred)\n            mask_all.append(valid_mask.numpy())\n            \n            # Collect IDs (which are byte strings)\n            doc_id_all.append(doc_id) # No .numpy() needed if already numpy array\n            veh_id_all.append(veh_id) # No .numpy() needed if already numpy array\n    \n        return (\n            np.concatenate(y_true_all, axis=0),\n            np.concatenate(y_pred_all, axis=0),\n            np.concatenate(mask_all, axis=0),\n            np.concatenate(doc_id_all, axis=0), # Concatenated array of sequence doc IDs\n            np.concatenate(veh_id_all, axis=0), # Concatenated array of sequence veh IDs\n        )\n    \n    def collect_predictions2(model, dataset):\n        y_true_all = []\n        y_pred_all = []\n        mask_all = []\n    \n        for (x, attn_mask), y, valid_mask in dataset:\n            y_pred = model.predict_on_batch((x, attn_mask))\n            y_true_all.append(y.numpy())\n            y_pred_all.append(y_pred)\n            mask_all.append(valid_mask.numpy())\n    \n        return (\n            np.concatenate(y_true_all, axis=0),\n            np.concatenate(y_pred_all, axis=0),\n            np.concatenate(mask_all, axis=0),\n        )\n\n    def F1_eval(model,test_ds,test_ds2):\n        # --- 1. THRESHOLD FINDING STAGE ---\n        \n        # Collect predictions - UPDATED to unpack 5 return values\n        y_test_thr, y_pred_thr, mask_thr = collect_predictions2(model, test_ds)\n        \n        # Flatten\n        y_test_thr_flat  = y_test_thr.reshape(-1, NUM_LABELS)\n        y_pred_thr_flat  = y_pred_thr.reshape(-1, NUM_LABELS)\n        mask_thr_flat    = mask_thr.reshape(-1) > 0.5\n        \n        # Apply mask \n        y_test_thr_clean = y_test_thr_flat[mask_thr_flat]\n        y_pred_thr_clean = y_pred_thr_flat[mask_thr_flat]\n        \n        # --- Threshold finding logic ---\n        best_thresholds = []\n        \n        for i in range(NUM_LABELS):\n            best_f1, best_t = 0.0, 0.5\n        \n            for t in np.linspace(0.1, 0.9, 17):\n                y_bin = (y_pred_thr_clean[:, i] > t).astype(int)\n                f1 = f1_score(\n                    y_test_thr_clean[:, i],\n                    y_bin,\n                    zero_division=0\n                )\n        \n                if f1 > best_f1:\n                    best_f1, best_t = f1, t\n        \n            best_thresholds.append(best_t)\n        \n        # --- 2. FINAL METRICS STAGE ---\n        \n        # Collect FINAL test predictions - UPDATED to unpack 5 return values\n        y_test2, y_pred_test2, mask_test2, doc_ids_test2, veh_ids_test2 = collect_predictions(model, test_ds2)\n        \n        # Flatten\n        y_test2_flat  = y_test2.reshape(-1, NUM_LABELS)\n        y_pred_test2_flat = y_pred_test2.reshape(-1, NUM_LABELS)\n        mask_test2_flat = mask_test2.reshape(-1) > 0.5\n        \n        # Apply mask\n        y_test2_clean = y_test2_flat[mask_test2_flat]\n        y_pred_test2_clean = y_pred_test2_flat[mask_test2_flat]\n        \n        # Calculate flattened binary predictions (for classification report)\n        y_pred_bin = np.zeros_like(y_pred_test2_clean, dtype=int)\n        for i, t in enumerate(best_thresholds):\n            y_pred_bin[:, i] = (y_pred_test2_clean[:, i] > t).astype(int)\n        \n        # --- FIX: DEFINE SEQUENCE-LEVEL BINARY PREDICTIONS ---\n        # The pretty_transition_report needs predictions in the original sequence shape (N_seq, seq_len, NUM_LABELS)\n        \n        # 1. Create a broadcastable array of thresholds\n        threshold_array = np.array(best_thresholds).reshape(1, 1, NUM_LABELS)\n        \n        # 2. Apply thresholds to the unflattened predictions\n        y_pred_bin_seq = (y_pred_test2 > threshold_array).astype(int)\n        # --- END FIX ---\n        report_dict = classification_report(\n            y_test2_clean.astype(int),\n            y_pred_bin,\n            target_names=LABEL_COLS,\n            zero_division=0,\n            output_dict=True\n        )\n        h_loss = hamming_loss(y_test2_clean, y_pred_bin)\n        macro_f1 = report_dict.get('macro avg', {}).get('f1-score')\n        return h_loss,macro_f1\n\n    #---------------F1 Score evaluation end---#\n    \n    fine_tune_seq_len = 192  # Must match PRE-TRAINED model length\n    \n    custom_objects = {\n    \"PositionalEmbedding\": PositionalEmbedding,\n    \"WarmUpCosine\": WarmUpCosine,\n    \"padfloat_to_attnbool\": padfloat_to_attnbool,\n    \"LossScaleOptimizer\": tf.keras.mixed_precision.LossScaleOptimizer,\n    }\n    \n    train_tfr =  [\"/kaggle/input/tfrecords-finetuning-192/train.tfrecord\"] # Ensure path matches your write_split output\n    val_tfr   =  [\"/kaggle/input/tfrecords-finetuning-192/val.tfrecord\"]\n    test_tfr   = [\"/kaggle/input/tfrecords-finetuning-192/test.tfrecord\"]\n    test_tfr2 =  [\"/kaggle/input/tfrecords-testing-192-doc-veh/test.tfrecord\"]\n  \n\n    import time\n    s=time.time()\n    # b=[4,8,16,32]\n    # b=[8]\n    c=[(4,4)]\n    for a,b in c:\n        BATCH_SIZE = 4\n        EPOCHS = 100\n    \n        train_ds = make_dataset(train_tfr, BATCH_SIZE, training=True,drop_remainder=True)\n        val_ds   = make_dataset(val_tfr, BATCH_SIZE, training=True,drop_remainder=False)\n        test_ds   = make_dataset(test_tfr, BATCH_SIZE, training=False,drop_remainder=False)\n        test_ds2   = make_dataset2(test_tfr2, BATCH_SIZE, training=False,drop_remainder=False)\n        \n        \n        SEEDS = [42]\n        # SEEDS = [42]\n        \n        results = []\n        for seed in SEEDS:\n            tf.random.set_seed(seed)\n            np.random.seed(seed)\n            pretrained = tf.keras.models.load_model(\n                \"/kaggle/input/bert-final-2/bert_stage3_e11_t1765702225.keras\",\n                custom_objects=custom_objects, \n                compile=False\n            )\n            \n            encoder_lora = add_lora_to_encoder(pretrained, rank=a, alpha=b)\n            model = build_finetune_model(encoder_lora)\n            \n            steps = EPOCHS * sum(1 for _ in train_ds)\n            lr = get_lr_schedule(steps)\n            \n            model.compile(\n                optimizer=tf.keras.optimizers.AdamW(lr), \n                # loss=\"binary_crossentropy\", \n                loss = MaskedBinaryCrossentropy(from_logits=False),\n                metrics=[metrics.AUC(name='auprc', curve='PR')]\n            )\n        \n            callback_list = [\n                tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True),                \n            ]\n            \n            history = model.fit(\n                train_ds,\n                validation_data=val_ds,\n                epochs=EPOCHS,\n                callbacks=callback_list,\n                verbose=0\n            )\n            test_results = model.evaluate(test_ds, verbose=0)\n            h_loss,macro_f1 = F1_eval(model,test_ds,test_ds2)\n            results.append({\n            # 'loss': test_results[0],\n            'auprc': test_results[1],\n            'hamming_loss': float(f\"{h_loss:.4f}\"),\n            'macro_f1': float(f\"{macro_f1:.4f}\")\n            })\n\n            \n        auprc_mean = np.mean([r['auprc'] for r in results])\n        auprc_std = np.std([r['auprc'] for r in results])\n        f1_mean = np.mean([r['macro_f1'] for r in results])\n        f1_std = np.std([r['macro_f1'] for r in results])\n        h_mean = np.mean([r['hamming_loss'] for r in results])\n        h_std = np.std([r['hamming_loss'] for r in results])\n        print(\"config\",b)\n        print(f\"AUPRC: {auprc_mean:.4f} Â± {auprc_std:.4f}\")\n        print(f\"f1: {f1_mean:.4f} Â± {f1_std:.4f}\")\n        print(f\"Hamming loss: {h_mean:.4f} Â± {h_std:.4f}\")\n        \n        \n        print(\"trainable_weights\",sum(w.numpy().size for w in model.trainable_weights))\n        e = time.time()\n        print(f\"Training finished. Elapsed time: {(e-s)/60:.2f} minutes\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation of the fine-tuned BERT","metadata":{}},{"cell_type":"code","source":"if USE_FINE_TUNE:\n    def collect_predictions(model, dataset):\n        \"\"\"\n        Collects predictions from the model, now also collecting document and vehicle IDs.\n        \"\"\"\n        y_true_all = []\n        y_pred_all = []\n        mask_all = []\n        doc_id_all = [] # New list for Document IDs\n        veh_id_all = [] # New list for Vehicle IDs\n    \n        # NOTE: The loop now unpacks 5 values from the dataset element\n        for (x, attn_mask), y, valid_mask, doc_id, veh_id in dataset:\n            y_pred = model.predict_on_batch((x, attn_mask))\n            \n            y_true_all.append(y.numpy())\n            y_pred_all.append(y_pred)\n            mask_all.append(valid_mask.numpy())\n            \n            # Collect IDs (which are byte strings)\n            doc_id_all.append(doc_id) # No .numpy() needed if already numpy array\n            veh_id_all.append(veh_id) # No .numpy() needed if already numpy array\n    \n        return (\n            np.concatenate(y_true_all, axis=0),\n            np.concatenate(y_pred_all, axis=0),\n            np.concatenate(mask_all, axis=0),\n            np.concatenate(doc_id_all, axis=0), # Concatenated array of sequence doc IDs\n            np.concatenate(veh_id_all, axis=0), # Concatenated array of sequence veh IDs\n        )\n    \n    def collect_predictions2(model, dataset):\n        y_true_all = []\n        y_pred_all = []\n        mask_all = []\n    \n        for (x, attn_mask), y, valid_mask in dataset:\n            y_pred = model.predict_on_batch((x, attn_mask))\n            y_true_all.append(y.numpy())\n            y_pred_all.append(y_pred)\n            mask_all.append(valid_mask.numpy())\n    \n        return (\n            np.concatenate(y_true_all, axis=0),\n            np.concatenate(y_pred_all, axis=0),\n            np.concatenate(mask_all, axis=0),\n        )\n    \n    # --- 1. THRESHOLD FINDING STAGE ---\n    \n    # Collect predictions - UPDATED to unpack 5 return values\n    y_test_thr, y_pred_thr, mask_thr = collect_predictions2(model, test_ds)\n    \n    # Flatten\n    y_test_thr_flat  = y_test_thr.reshape(-1, NUM_LABELS)\n    y_pred_thr_flat  = y_pred_thr.reshape(-1, NUM_LABELS)\n    mask_thr_flat    = mask_thr.reshape(-1) > 0.5\n    \n    # Apply mask \n    y_test_thr_clean = y_test_thr_flat[mask_thr_flat]\n    y_pred_thr_clean = y_pred_thr_flat[mask_thr_flat]\n    \n    # --- Threshold finding logic ---\n    best_thresholds = []\n    \n    for i in range(NUM_LABELS):\n        best_f1, best_t = 0.0, 0.5\n    \n        for t in np.linspace(0.1, 0.9, 17):\n            y_bin = (y_pred_thr_clean[:, i] > t).astype(int)\n            f1 = f1_score(\n                y_test_thr_clean[:, i],\n                y_bin,\n                zero_division=0\n            )\n    \n            if f1 > best_f1:\n                best_f1, best_t = f1, t\n    \n        best_thresholds.append(best_t)\n    \n    print(\"Optimal thresholds (from test_ds):\")\n    print(best_thresholds)\n    \n    # --- 2. FINAL METRICS STAGE ---\n    \n    # Collect FINAL test predictions - UPDATED to unpack 5 return values\n    y_test2, y_pred_test2, mask_test2, doc_ids_test2, veh_ids_test2 = collect_predictions(model, test_ds2)\n    \n    # Flatten\n    y_test2_flat  = y_test2.reshape(-1, NUM_LABELS)\n    y_pred_test2_flat = y_pred_test2.reshape(-1, NUM_LABELS)\n    mask_test2_flat = mask_test2.reshape(-1) > 0.5\n    \n    # Apply mask\n    y_test2_clean = y_test2_flat[mask_test2_flat]\n    y_pred_test2_clean = y_pred_test2_flat[mask_test2_flat]\n    \n    # Calculate flattened binary predictions (for classification report)\n    y_pred_bin = np.zeros_like(y_pred_test2_clean, dtype=int)\n    for i, t in enumerate(best_thresholds):\n        y_pred_bin[:, i] = (y_pred_test2_clean[:, i] > t).astype(int)\n    \n    # --- FIX: DEFINE SEQUENCE-LEVEL BINARY PREDICTIONS ---\n    # The pretty_transition_report needs predictions in the original sequence shape (N_seq, seq_len, NUM_LABELS)\n    \n    # 1. Create a broadcastable array of thresholds\n    threshold_array = np.array(best_thresholds).reshape(1, 1, NUM_LABELS)\n    \n    # 2. Apply thresholds to the unflattened predictions\n    y_pred_bin_seq = (y_pred_test2 > threshold_array).astype(int)\n    # --- END FIX ---\n    \n    print(\"FINAL TEST RESULTS (test_ds2 - Frame-level)\")\n    print(classification_report(\n        y_test2_clean.astype(int),\n        y_pred_bin,\n        target_names=LABEL_COLS,\n        zero_division=0\n    ))\n    \n    print(\"Hamming Loss:\", hamming_loss(y_test2_clean, y_pred_bin))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if USE_FINE_TUNE:\n    dependent_pairs = {\n        \"Cut-in in front of ego vehicle\": [\n            \"Lead vehicle accelerating\",\n            \"Lead vehicle cruising\",\n            \"Lead vehicle decelerating\"\n        ],\n        \"Cut-out in front of ego vehicle\": [\n            \"Ego vehicle driving in lane without lead vehicle\",\n            \"Lead vehicle cruising\",\n            \"Lead vehicle decelerating\",\n            \"Lead vehicle accelerating\"\n        ],\n        \"Ego merging into an occupied lane\": [\n            \"Lead vehicle cruising\",\n            \"Lead vehicle decelerating\",\n            \"Lead vehicle accelerating\"\n        ],\n        \"Ego vehicle performing lane change\": [\n            \"Ego vehicle driving in lane without lead vehicle\"\n        ],\n        \"Ego vehicle performing lane change with vehicle behind\": [\n            \"Ego vehicle driving in lane without lead vehicle\"\n        ],\n        \"Ego vehicle driving in lane without lead vehicle\": [\n            \"Ego vehicle approaching slower lead vehicle\",\n            \"Lead vehicle accelerating\",\n            \"Lead vehicle cruising\",\n            \"Lead vehicle decelerating\"\n        ]\n    }\n    \n    def compute_boundary_metrics_fast(y_true, y_pred, length=None):\n        \"\"\"Computes Mean Absolute Boundary Error (MABE) for a single sequence.\"\"\"\n        true_boundaries = np.where((y_true[1:] == 1) & (y_true[:-1] == 0))[0] + 1\n        pred_boundaries = np.where((y_pred[1:] == 1) & (y_pred[:-1] == 0))[0] + 1\n        if len(true_boundaries) == 0 or len(pred_boundaries) == 0:\n            return np.nan\n        diffs = []\n        for tb in true_boundaries:\n            idx = np.searchsorted(pred_boundaries, tb)\n            cands = []\n            if idx > 0: cands.append(abs(tb - pred_boundaries[idx - 1]))\n            if idx < len(pred_boundaries): cands.append(abs(tb - pred_boundaries[idx]))\n            diffs.append(min(cands))\n        mabe = float(np.mean(diffs))\n        if length is not None:\n            return mabe / length  # <= normalization step\n        return mabe\n    \n    def pretty_transition_report(\n        y_sequences: np.ndarray,\n        y_pred_proba: np.ndarray = None,\n        doc_ids: np.ndarray = None,\n        veh_ids: np.ndarray = None,\n        labels: List[str] = None,\n        best_thresholds: List[float] = None,\n        y_pred_bin: np.ndarray = None,\n        dependent_pairs: Dict[str, List[str]] = None,\n        match_tolerance_prev: int = 1,\n        allow_curr_offset: int = 1\n    ) -> Dict[str, Any]:\n        \"\"\"Compute boundary MABE, missed scenario-per-vehicle, and transition MABE+missed per dependent pair.\n    \n        Returns dict with 'boundary_summary', 'missed_percent_vehicle', 'transition_summary', etc.\n        \"\"\"\n        # Basic checks\n        assert labels is not None, \"Provide labels list\"\n        n_seq, seq_len, n_labels = y_sequences.shape\n    \n        # Build binary predictions if not provided (this path is redundant with the fix but kept)\n        if y_pred_bin is None:\n            if y_pred_proba is None or best_thresholds is None:\n                raise ValueError(\"Either y_pred_bin OR (y_pred_proba and best_thresholds) must be provided.\")\n            thr = np.array(best_thresholds).reshape(1, 1, -1)\n            y_pred_bin = (y_pred_proba.astype(np.float32) > thr).astype(np.int32)\n        else:\n            y_pred_bin = y_pred_bin.astype(np.int32)\n    \n        # ensure y_sequences ints\n        y_sequences = y_sequences.astype(np.int32)\n    \n        # 1) Build mapping (doc,veh) -> ordered list of sequence indices\n        seqs_by_vehicle = defaultdict(list)\n        for idx, (d, v) in enumerate(zip(doc_ids, veh_ids)):\n            seqs_by_vehicle[(d, v)].append(idx)\n    \n        # 2) Boundary MABE per label averaged across vehicles\n        boundary_results = defaultdict(list)\n        for (d, v), seq_indices in seqs_by_vehicle.items():\n            y_true_vehicle = np.concatenate([y_sequences[i] for i in seq_indices], axis=0)\n            y_pred_vehicle = np.concatenate([y_pred_bin[i] for i in seq_indices], axis=0)\n            for i, lab in enumerate(labels):\n                mabe = compute_boundary_metrics_fast(y_true_vehicle[:, i], y_pred_vehicle[:, i],length=len(y_true_vehicle))\n                if not np.isnan(mabe):\n                    boundary_results[lab].append(mabe)\n        boundary_summary = {lab: float(np.mean(vals)) if len(vals) > 0 else np.nan\n                                 for lab, vals in boundary_results.items() for _ in [0]}\n        # preserve order and fill missing labels\n        boundary_summary = {lab: (boundary_summary.get(lab, np.nan)) for lab in labels}\n        avg_mabe = float(np.nanmean([v for v in boundary_summary.values() if not np.isnan(v)])) if any(\n            not np.isnan(v) for v in boundary_summary.values()) else np.nan\n    \n        # 3) Missed Scenario Detection Rate per vehicle\n        missed_counts_vehicle = defaultdict(int)\n        total_support_vehicle = defaultdict(int)\n        for (d, v), seq_indices in seqs_by_vehicle.items():\n            y_true_vehicle = np.concatenate([y_sequences[i] for i in seq_indices], axis=0)\n            y_pred_vehicle = np.concatenate([y_pred_bin[i] for i in seq_indices], axis=0)\n            for i, lab in enumerate(labels):\n                if y_true_vehicle[:, i].sum() > 0:\n                    total_support_vehicle[lab] += 1\n                    if y_pred_vehicle[:, i].sum() == 0:\n                        missed_counts_vehicle[lab] += 1\n        missed_percent_vehicle = {lab: (100.0 * missed_counts_vehicle.get(lab, 0) / total_support_vehicle.get(lab, 1))\n                                  if total_support_vehicle.get(lab, 0) > 0 else np.nan for lab in labels}\n    \n        # Simplified per-vehicle transition evaluation WITHOUT match_tolerance_prev\n        label2idx = {l: i for i, l in enumerate(labels)}\n        transition_results = defaultdict(list)     # per-pair list of per-instance distances\n        transition_support = defaultdict(int)      # number of true transition instances (actual)\n        missed_transitions = defaultdict(int)      # number of true instances missed\n        \n        allow_curr_offset = 1  # allow predicted curr_start == pred_prev_end or pred_prev_end + 1\n        \n        def ends(idx_seq):    # 1->0 ends, returns indices of the frame after end\n            return np.where((idx_seq[:-1] == 1) & (idx_seq[1:] == 0))[0] + 1\n        \n        def starts(idx_seq): # 0->1 starts, returns indices\n            return np.where((idx_seq[1:] == 1) & (idx_seq[:-1] == 0))[0] + 1\n        \n        # loop per vehicle\n        for (d, v), seq_indices in seqs_by_vehicle.items():\n            # reconstruct whole vehicle timeline\n            y_true_vehicle = np.concatenate([y_sequences[i] for i in seq_indices], axis=0)\n            y_pred_vehicle = np.concatenate([y_pred_bin[i] for i in seq_indices], axis=0)\n            vehicle_len = len(y_true_vehicle)\n        \n            for prev_label, next_labels in (dependent_pairs or {}).items():\n                if prev_label not in label2idx:\n                    continue\n                p_idx = label2idx[prev_label]\n                pred_prev_end = ends(y_pred_vehicle[:, p_idx])\n        \n                for curr_label in next_labels:\n                    if curr_label not in label2idx:\n                        continue\n                    c_idx = label2idx[curr_label]\n        \n                    true_prev_end = ends(y_true_vehicle[:, p_idx])\n                    true_curr_start = starts(y_true_vehicle[:, c_idx])\n                    pred_curr_start = starts(y_pred_vehicle[:, c_idx])\n        \n                    # build predicted pairs in vehicle: (pred_prev_end -> pred_curr_start) where curr==prev or prev+1\n                    pred_pairs_curr = []\n                    if len(pred_prev_end) > 0 and len(pred_curr_start) > 0:\n                        for p in pred_prev_end:\n                            matches = pred_curr_start[(pred_curr_start == p) | (pred_curr_start == p + allow_curr_offset)]\n                            if len(matches) > 0:\n                                pred_pairs_curr.extend(list(matches))  # collect predicted curr_starts\n        \n                    # For each true prev_end that actually has a following true curr_start (0 or +1)\n                    for te in true_prev_end:\n                        if not (np.any(true_curr_start == te) or np.any(true_curr_start == te + 1)):\n                            continue  # not a true prev->curr instance\n        \n                        transition_support[(prev_label, curr_label)] += 1\n        \n                        if len(pred_pairs_curr) == 0:\n                            # no predicted dependent pairs anywhere in this vehicle -> missed\n                            missed_transitions[(prev_label, curr_label)] += 1\n                            continue\n        \n                        # choose the true curr_start for this instance (prefer te, else te+1)\n                        true_cs_cands = true_curr_start[(true_curr_start == te) | (true_curr_start == te + 1)]\n                        if len(true_cs_cands) > 0:\n                            true_cs = int(true_cs_cands[0])\n                        else:\n                            # fallback: nearest true curr start\n                            true_cs = int(true_curr_start[np.argmin(np.abs(true_curr_start - te))]) if len(true_curr_start) > 0 else None\n        \n                        if true_cs is None:\n                            missed_transitions[(prev_label, curr_label)] += 1\n                            continue\n        \n                        # compute distance to nearest predicted curr_start among vehicle's predicted pairs\n                        pred_cs_arr = np.array(pred_pairs_curr)\n                        best_dist = np.min(np.abs(pred_cs_arr - true_cs)) / vehicle_len\n                        transition_results[(prev_label, curr_label)].append(best_dist)\n        \n        # Final per-pair MABE (mean of per-instance distances)\n        transition_summary = {\n            pair: float(np.mean(dists)) if len(dists) > 0 else 0\n            for pair, dists in transition_results.items()\n        }\n    \n        # compute missed rates and print results\n        print(\"\\nBoundary metrics per class (averaged across vehicles):\")\n        for lab in labels:\n            mabe = boundary_summary.get(lab, np.nan)\n            if np.isnan(mabe):\n                print(f\"{lab}: MABE = n/a\")\n            else:\n                print(f\"{lab}: MABE = {mabe:.2f}\")\n        print(f\"\\nAverage MABE across all classes: {avg_mabe:.4f}\\n\")\n    \n        print(\"Missed Scenario Detection Rates (% of vehicles where class was present but never predicted):\")\n        for lab in labels:\n            pct = missed_percent_vehicle.get(lab, np.nan)\n            total_support = total_support_vehicle.get(lab, 0)\n            if np.isnan(pct):\n                print(f\"{lab}: n/a\")\n            else:\n                print(f\"{lab}: {pct:.2f}% missed (support: {total_support})\")\n        avg_miss_rate_vehicle = float(np.nanmean([v for v in missed_percent_vehicle.values() if not np.isnan(v)])) if any(not np.isnan(v) for v in missed_percent_vehicle.values()) else np.nan\n        print(f\"\\nAverage Miss Rate Across All Classes (per vehicle): {avg_miss_rate_vehicle:.2f}%\\n\")\n    \n        # overall stats\n        avg_transition_mabe = float(np.nanmean([v for v in transition_summary.values() if not np.isnan(v)])) if len(transition_summary) > 0 else np.nan\n        total_transitions = sum(transition_support.values())\n        total_missed = sum(missed_transitions.values())\n        overall_miss_rate = (total_missed / total_transitions * 100) if total_transitions > 0 else 0.0\n    \n        # --- 5) Summarize transition metrics ---\n        print(\"\\nTransition-based MABE (evaluated per vehicle):\")\n        has_transitions = False\n        for prev_label, next_labels in dependent_pairs.items():\n            for curr_label in next_labels:\n                pair = (prev_label, curr_label)\n                total = transition_support.get(pair, 0)\n                if total == 0:\n                    continue  # skip transitions that never occurred in ground truth\n                \n                has_transitions = True\n                missed = missed_transitions.get(pair, 0)\n                mabe_vals = transition_results.get(pair, [])\n                mean_mabe = np.mean(mabe_vals) if len(mabe_vals) > 0 else np.nan\n                missed_rate = (missed / total * 100) if total > 0 else 0.0\n                \n                print(f\"{prev_label} -> {curr_label}: \"\n                      f\"MABE = {mean_mabe:.4f} | Support = {total} | \"\n                      f\"Missed = {missed} ({missed_rate:.1f}%)\")\n        \n        if has_transitions:\n            avg_mabe_pairs = np.nanmean([np.mean(v) for v in transition_results.values() if len(v) > 0])\n            total_transitions = sum(transition_support.values())\n            total_missed = sum(missed_transitions.values())\n            avg_miss_rate_pairs = (total_missed / total_transitions * 100) if total_transitions > 0 else 0\n            print(f\"\\nAverage Transition MABE Across All Pairs: {avg_mabe_pairs:.4f}\")\n            print(f\"Overall Missed Transition Detection Rate (per instance): {avg_miss_rate_pairs:.2f}%\")\n        else:\n            print(\"\\nâš ï¸ No valid dependent transitions detected in the test set.\")\n    \n        # Return structured results\n        return {\n            \"boundary_summary\": boundary_summary,\n            \"avg_mabe\": avg_mabe,\n            \"missed_percent_vehicle\": missed_percent_vehicle,\n            \"avg_miss_rate_vehicle\": avg_miss_rate_vehicle,\n            \"support_counts_vehicle\": {lab: total_support_vehicle.get(lab, 0) for lab in labels},\n            \"missed_counts_vehicle\": {lab: missed_counts_vehicle.get(lab, 0) for lab in labels},\n            \"transition_summary\": transition_summary,\n            \"transition_support\": dict(transition_support),\n            \"missed_transitions\": dict(missed_transitions),\n            \"avg_transition_mabe\": avg_transition_mabe,\n            \"overall_miss_rate\": overall_miss_rate\n        }\n    \n    # -------------------------------\n    # CORRECTED USAGE:\n    # -------------------------------\n    \n    print(\"\\n--- Running Transition Report ---\")\n    result = pretty_transition_report(\n        # Use the unflattened ground truth sequences as y_sequences\n        y_sequences=y_test2,\n        # Use the unflattened probability sequences as y_pred_proba\n        y_pred_proba=y_pred_test2,\n        # Pass the new ID arrays extracted from collect_predictions\n        doc_ids=doc_ids_test2,\n        veh_ids=veh_ids_test2,\n        labels=LABEL_COLS,\n        best_thresholds=best_thresholds,\n        y_pred_bin=y_pred_bin_seq, # THIS VARIABLE IS NOW DEFINED ABOVE\n        dependent_pairs=dependent_pairs\n    )\n    \n    print(\"\\n--- Full Metrics Report Generated ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}